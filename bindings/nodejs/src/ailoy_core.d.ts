/* auto-generated by NAPI-RS */
/* eslint-disable */
/**
 * The Agent is the central orchestrator that connects the **language model**, **tools**, and **knowledge** components.
 * It manages the entire reasoning and action loop, coordinating how each subsystem contributes to the final response.
 *
 * In essence, the Agent:
 *
 * - Understands user input
 * - Interprets structured responses from the language model (such as tool calls)
 * - Executes tools as needed
 * - Retrieves and integrates contextual knowledge before or during inference
 *
 * # Public APIs
 * - `run_delta`: Runs a user query and streams incremental deltas (partial outputs)
 * - `run`: Runs a user query and returns a complete message once all deltas are accumulated
 *
 * ## Delta vs. Complete Message
 * A *delta* represents a partial piece of model output, such as a text fragment or intermediate reasoning step.
 * Deltas can be accumulated into a full message using the provided accumulation utilities.
 * This allows real-time streaming while preserving the ability to reconstruct the final structured result.
 *
 * See `MessageDelta`.
 *
 * # Components
 * - **Language Model**: Generates natural language and structured outputs. It interprets the conversation context and predicts the assistant’s next action.
 * - **Tool**: Represents external functions or APIs that the model can dynamically invoke. The `Agent` detects tool calls and automatically executes them during the reasoning loop.
 * - **Knowledge**: Provides retrieval-augmented reasoning by fetching relevant information from stored documents or databases. When available, the `Agent` enriches model input with these results before generating an answer.
 */
export declare class Agent {
  constructor(lm: LangModel, tools?: Array<Tool> | undefined | null);
  addTool(tool: Tool): void;
  addTools(tools: Array<Tool>): void;
  removeTool(toolName: string): void;
  removeTools(toolNames: Array<string>): void;
  setKnowledge(knowledge: Knowledge): void;
  removeKnowledge(): void;
  runDelta(
    messages: Messages,
    config?: AgentConfig | undefined | null
  ): MessageDeltaOutputIterator;
  run(
    messages: Messages,
    config?: AgentConfig | undefined | null
  ): MessageOutputIterator;
}

export declare class EmbeddingModel {
  static newLocal(
    modelName: string,
    deviceId?: number | undefined | null,
    progressCallback?: ((arg: CacheProgress) => void) | undefined | null
  ): Promise<EmbeddingModel>;
  infer(text: string): Promise<Embedding>;
}

export declare class Knowledge {
  static newVectorStore(
    store: VectorStore,
    embeddingModel: EmbeddingModel
  ): Knowledge;
  retrieve(
    query: string,
    config?: KnowledgeConfig | undefined | null
  ): Promise<Array<Document>>;
  asTool(): Tool;
}

export declare class LangModel {
  static newLocal(
    modelName: string,
    deviceId?: number | undefined | null,
    progressCallback?: ((arg: CacheProgress) => void) | undefined | null
  ): Promise<LangModel>;
  static newStreamAPI(
    spec: APISpecification,
    modelName: string,
    apiKey: string
  ): Promise<LangModel>;
  inferDelta(
    messages: Messages,
    tools?: Array<ToolDesc> | undefined | null,
    docs?: Array<Document> | undefined | null
  ): MessageDeltaOutputIterator;
  infer(
    messages: Messages,
    tools?: Array<ToolDesc> | undefined | null,
    docs?: Array<Document> | undefined | null
  ): MessageDeltaOutput;
}

export declare class MCPClient {
  static newStdio(command: string, args: Array<string>): Promise<MCPClient>;
  static newStreamableHttp(url: string): Promise<MCPClient>;
  get tools(): Array<Tool>;
}

export declare class MessageDeltaOutputIterator {
  [Symbol.asyncIterator](): this;
  next(): Promise<MessageDeltaOutputIteratorResult>;
}

export declare class MessageOutputIterator {
  [Symbol.asyncIterator](): this;
  next(): Promise<MessageOutputIteratorResult>;
}

export declare class Tool {
  static newBuiltin(kind: BuiltinToolKind): Tool;
  static newFunction(desc: ToolDesc, func: (arg: any) => Promise<any>): Tool;
  get description(): ToolDesc;
  run(args: any): Promise<any>;
}

export declare class VectorStore {
  static newFaiss(dim: number): Promise<VectorStore>;
  static newChroma(
    url: string,
    collectionName?: string | undefined | null
  ): Promise<VectorStore>;
  addVector(input: VectorStoreAddInput): Promise<string>;
  addVectors(inputs: Array<VectorStoreAddInput>): Promise<Array<string>>;
  getById(id: string): Promise<VectorStoreGetResult | null>;
  getByIds(ids: Array<string>): Promise<Array<VectorStoreGetResult>>;
  retrieve(
    queryEmbedding: Embedding,
    topK: number
  ): Promise<Array<VectorStoreRetrieveResult>>;
  batchRetrieve(
    queryEmbeddings: Array<Embedding>,
    topK: number
  ): Promise<Array<Array<VectorStoreRetrieveResult>>>;
  removeVector(id: string): Promise<void>;
  removeVectors(ids: Array<string>): Promise<void>;
  clear(): Promise<void>;
  count(): Promise<number>;
}

/**
 * Configuration for running the agent.
 *
 * See `InferenceConfig` and `KnowledgeConfig` for more details.
 */
export interface AgentConfig {
  inference?: InferenceConfig;
  knowledge?: KnowledgeConfig;
}

export type APISpecification =
  | "ChatCompletion"
  | "OpenAI"
  | "Gemini"
  | "Claude"
  | "Responses"
  | "Grok";

export type BuiltinToolKind = "terminal";

export interface CacheProgress {
  comment: string;
  current: number;
  total: number;
}

export interface Document {
  id: string;
  title?: string;
  text: string;
}

/** Provides a polyfill for LLMs that do not natively support the Document feature. */
export interface DocumentPolyfill {
  systemMessageTemplate?: string;
  queryMessageTemplate?: string;
}

export type Embedding = Float32Array;

/** Explains why a language model's streamed generation finished. */
export type FinishReason =
  | { type: "stop" }
  | { type: "length" }
  | { type: "tool_call" }
  | { type: "refusal"; reason: string };

export type Grammar =
  | { type: "plain" }
  | { type: "json" }
  | { type: "jsonschema"; schema: string }
  | { type: "regex"; regex: string }
  | { type: "cfg"; cfg: string };

export declare function imageFromBase64(data: string): Part;

export declare function imageFromBytes(data: Uint8Array): Part;

export declare function imageFromUrl(url: string): Part;

/**
 * Configuration parameters that control the behavior of model inference.
 *
 * `InferenceConfig` encapsulates all the configuration, controlling behavior of `LanguageModel`` inference.
 *
 * # Fields
 *
 * ## `document_polyfill`
 * Configuration describing how retrieved documents are embedded into the model input.
 * If `None`, it does not perform any polyfill, (ignoring documents).
 *
 * ## `think_effort`
 * Controls the model’s reasoning intensity.
 * In local models, `low`, `medium`, `high` is ignored.
 * In API models, it is up to it's API. See API parameters.
 *
 * Possible values: `disable`, `enable`, `low`, `medium`, `high`.
 *
 * ## `temperature`
 * Sampling temperature controlling randomness of output.
 * Lower values make output more deterministic; higher values increase diversity.
 *
 * ## `top_p`
 * Nucleus sampling parameter (probability mass cutoff).
 * Limits token sampling to a cumulative probability ≤ `top_p`.`
 *
 * ## `max_tokens`
 * Maximum number of tokens to generate for a single inference.
 *
 * ## `grammar`
 * Optional grammar constraint that restricts valid output forms.
 * Supported types include:
 * - `Plain`: unconstrained text
 * - `JSON`: ensures valid JSON output
 * - `JSONSchema { schema }`: validates JSON against the given schema
 * - `Regex { regex }`: constrains generation by a regular expression
 * - `CFG { cfg }`: uses a context-free grammar definition
 */
export interface InferenceConfig {
  documentPolyfill?: DocumentPolyfill;
  thinkEffort?: ThinkEffort;
  temperature?: number;
  topP?: number;
  maxTokens?: number;
  grammar?: Grammar;
}

export interface KnowledgeConfig {
  topK?: number;
}

/**
 * A chat message generated by a user, model, or tool.
 *
 * `Message` is the concrete, non-streaming container used by the application to store, transmit, or feed structured content into models or tools.
 * It can represent various kinds of messages, including user input, assistant responses, tool-call outputs, or signed *thinking* metadata.
 *
 * Note that many different kinds of messages can be produced.
 * For example, a language model may internally generate a `thinking` trace before emitting its final output, in order to improve reasoning accuracy.
 * In other cases, a model may produce *function calls* — structured outputs that instruct external tools to perform specific actions.
 *
 * This struct is designed to handle all of these situations in a unified way.
 *
 * # Example
 *
 * ## Rust
 * ```rust
 * let msg = Message::new(Role::User).with_contents([Part::text("hello")]);
 * assert_eq!(msg.role, Role::User);
 * assert_eq!(msg.contents.len(), 1);
 * ```
 */
export interface Message {
  /** Author of the message. */
  role: Role;
  /** Primary parts of the message (e.g., text, image, value, or function). */
  contents: Array<Part>;
  /** Optional stable identifier for deduplication or threading. */
  id?: string;
  /** Internal “thinking” text used by some models before producing final output. */
  thinking?: string;
  /** Tool-call parts emitted alongside the main contents. */
  tool_calls?: Array<Part>;
  /**
   * Optional signature for the `thinking` field.
   *
   * This is only applicable to certain LLM APIs that require a signature as part of the `thinking` payload.
   */
  signature?: string;
}

/**
 * A streaming, incremental update to a [`Message`].
 *
 * `MessageDelta` accumulates partial outputs (text chunks, tool-call fragments, IDs, signatures, etc.) until they can be materialized as a full [`Message`].
 * It implements [`Delta`] to support accumulation.
 *
 * # Accumulation Rules
 * - `role`: merging two distinct roles fails.
 * - `thinking`: concatenated in arrival order.
 * - `contents`/`tool_calls`: last element is accumulated with the incoming delta when both are compatible (e.g., Text+Text, Function+Function with matching ID policy), otherwise appended as a new fragment.
 * - `id`/`signature`: last-writer-wins.
 *
 * # Finalization
 * - `finish()` converts the accumulated deltas into a fully-formed [`Message`].
 *   Fails if required fields (e.g., `role`) are missing or inner deltas cannot be finalized.
 *
 * # Examples
 * ```rust
 * let d1 = MessageDelta::new().with_role(Role::Assistant).with_contents([PartDelta::Text { text: "Hel".into() }]);
 * let d2 = MessageDelta::new().with_contents([PartDelta::Text { text: "lo".into() }]);
 *
 * let merged = d1.accumulate(d2).unwrap();
 * let msg = merged.finish().unwrap();
 * assert_eq!(msg.contents[0].as_text().unwrap(), "Hello");
 * ```
 */
export interface MessageDelta {
  role?: Role;
  contents: Array<PartDelta>;
  id?: string;
  thinking?: string;
  tool_calls: Array<PartDelta>;
  signature?: string;
}

/**
 * A container for a streamed message delta and its termination signal.
 *
 * During streaming, `delta` carries the incremental payload; once a terminal
 * condition is reached, `finish_reason` may be populated to explain why.
 *
 * # Examples
 * ```rust
 * let mut out = MessageOutput::new();
 * out.delta = MessageDelta::new().with_role(Role::Assistant).with_contents([PartDelta::Text { text: "Hi".into() }]);
 * assert!(out.finish_reason.is_none());
 * ```
 *
 * # Lifecycle
 * - While streaming: `finish_reason` is typically `None`.
 * - On completion: `finish_reason` is set; callers can then `finish()` the delta to obtain a concrete [`Message`].
 */
export interface MessageDeltaOutput {
  delta: MessageDelta;
  finish_reason?: FinishReason;
}

export interface MessageDeltaOutputIteratorResult {
  value: MessageDeltaOutput;
  done: boolean;
}

export interface MessageOutput {
  message: Message;
  finish_reason: FinishReason;
}

export interface MessageOutputIteratorResult {
  value: MessageOutput;
  done: boolean;
}

export type Messages = Array<Message | SingleTextMessage> | string;

export type Metadata = Record<string, any>;

/**
 * Represents a semantically meaningful content unit exchanged between the model and the user.
 *
 * Conceptually, each `Part` encapsulates a piece of **data** that contributes
 * to a chat message — such as text, a function invocation, or an image.
 *
 * For example, a single message consisting of a sequence like
 * `(text..., image, text...)` is represented as a `Message` containing
 * an array of three `Part` elements.
 *
 * Note that a `Part` does **not** carry "intent", such as "reasoning" or "tool call".
 * These higher-level semantics are determined by the context of a [`Message`].
 *
 * # Example
 *
 * ## Rust
 * ```rust
 * let part = Part::text("Hello, world!");
 * assert!(part.is_text());
 * ```
 */
export type Part =
  | { type: "text"; text: string }
  | { type: "function"; id?: string; function: PartFunction }
  | { type: "value"; value: any }
  | { type: "image"; image: PartImage };

/**
 * Represents a partial or incremental update (delta) of a [`Part`].
 *
 * This type enables composable, streaming updates to message parts.
 * For example, text may be produced token-by-token, or a function call
 * may be emitted gradually as its arguments stream in.
 *
 * # Example
 *
 * ## Rust
 * ```rust
 * let d1 = PartDelta::Text { text: "Hel".into() };
 * let d2 = PartDelta::Text { text: "lo".into() };
 * let merged = d1.accumulate(d2).unwrap();
 * assert_eq!(merged.to_text().unwrap(), "Hello");
 * ```
 *
 * # Error Handling
 * Accumulation or finalization may return an error if incompatible deltas
 * (e.g. mismatched function IDs) are combined or invalid JSON arguments are given.
 */
export type PartDelta =
  | { type: "text"; text: string }
  | { type: "function"; id?: string; function: PartDeltaFunction }
  | { type: "value"; value: any }
  | { type: "null" };

/**
 * Represents an incremental update (delta) of a function part.
 *
 * This type is used during streaming or partial message generation, when function calls are being streamed as text chunks or partial JSON fragments.
 *
 * # Variants
 * * `Verbatim(String)` — Raw text content, typically a partial JSON fragment.
 * * `WithStringArgs { name, arguments }` — Function name and its serialized arguments as strings.
 * * `WithParsedArgs { name, arguments }` — Function name and parsed arguments as a `Value`.
 *
 * # Use Case
 * When the model streams out a function call response (e.g., `"function_call":{"name":...}`),
 * the incremental deltas can be accumulated until the full function payload is formed.
 *
 * # Example
 * ```rust
 * let delta = PartDeltaFunction::WithStringArgs {
 *     name: "translate".into(),
 *     arguments: r#"{"text":"hi"}"#.into(),
 * };
 * ```
 */
export type PartDeltaFunction =
  | { type: "verbatim"; text: string }
  | { type: "with_string_args"; name: string; arguments: string }
  | { type: "with_parsed_args"; name: string; arguments: any };

/** Represents a function call contained within a message part. */
export interface PartFunction {
  /** The name of the function */
  name: string;
  /** The arguments of the function, usually represented as a JSON object. */
  arguments: any;
}

/**
 * Represents the image data contained in a [`Part`].
 *
 * `PartImage` provides structured access to image data.
 * Currently, it only implments "binary" types.
 *
 * # Example
 * ```rust
 * let part = Part::image_binary(640, 480, "rgb", (0..640*480*3).map(|i| (i % 255) as u8)).unwrap();
 *
 * if let Some(img) = part.as_image() {
 *     assert_eq!(img.height(), 640);
 *     assert_eq!(img.width(), 480);
 * }
 * ```
 */
export type PartImage =
  | {
      type: "binary";
      height: number;
      width: number;
      colorspace: PartImageColorspace;
      data: Buffer;
    }
  | { type: "url"; url: string };

/**
 * Represents the color space of an image part.
 *
 * This enum defines the supported pixel formats of image data. It determines
 * how many channels each pixel has and how the image should be interpreted.
 *
 * # Examples
 * ```rust
 * let c = PartImageColorspace::RGB;
 * assert_eq!(c.channel(), 3);
 * ```
 */
export type PartImageColorspace =
  /** Single-channel grayscale image */
  | "grayscale"
  /** Three-channel color image without alpha     */
  | "rgb"
  /** Four-channel color image with alpha */
  | "rgba";

/** The author of a message (or streaming delta) in a chat. */
export type Role =
  /** System instructions and constraints provided to the assistant. */
  | "system"
  /** Content authored by the end user. */
  | "user"
  /** Content authored by the assistant/model. */
  | "assistant"
  /** Outputs produced by external tools/functions */
  | "tool";

/**
 * A simplified form of [Message] for concise definition.
 * All other members are identical to [Message], but `contents` is a `String` instead of `Vec<Part>`.
 * This can be converted to Message via `.into()`.
 */
export interface SingleTextMessage {
  /** Author of the message. */
  role: Role;
  /** Primary part of message in text. */
  contents: string;
  /** Optional stable identifier for deduplication or threading. */
  id?: string;
  /** Internal “thinking” text used by some models before producing final output. */
  thinking?: string;
  /** Tool-call parts emitted alongside the main contents. */
  tool_calls?: Array<Part>;
  /**
   * Optional signature for the `thinking` field.
   *
   * This is only applicable to certain LLM APIs that require a signature as part of the `thinking` payload.
   */
  signature?: string;
}

export type ThinkEffort = "disable" | "enable" | "low" | "medium" | "high";

/**
 * Describes a **tool** (or function) that a language model can invoke.
 *
 * `ToolDesc` defines the schema, behavior, and input/output specification of a callable
 * external function, allowing an LLM to understand how to use it.
 *
 * The primary role of this struct is to describe to the LLM what a *tool* does,
 * how it can be invoked, and what input (`parameters`) and output (`returns`) schemas it expects.
 *
 * The format follows the same **schema conventions** used by Hugging Face’s
 * `transformers` library, as well as APIs such as *OpenAI* and *Anthropic*.
 * The `parameters` and `returns` fields are typically defined using **JSON Schema**.
 *
 * We provide a builder [`ToolDescBuilder`] helper for convenient and fluent construction.
 * Please refer to [`ToolDescBuilder`].
 *
 * # Example
 * ```rust
 * use crate::value::{ToolDescBuilder, to_value};
 *
 * let desc = ToolDescBuilder::new("temperature")
 *     .description("Get the current temperature for a given city")
 *     .parameters(to_value!({
 *         "type": "object",
 *         "properties": {
 *             "location": {
 *                 "type": "string",
 *                 "description": "The city name"
 *             },
 *             "unit": {
 *                 "type": "string",
 *                 "description": "Temperature unit (default: Celsius)",
 *                 "enum": ["Celsius", "Fahrenheit"]
 *             }
 *         },
 *         "required": ["location"]
 *     }))
 *     .returns(to_value!({
 *         "type": "number"
 *     }))
 *     .build();
 *
 * assert_eq!(desc.name, "temperature");
 * ```
 */
export interface ToolDesc {
  /** The unique name of the tool or function. */
  name: string;
  /** A natural-language description of what the tool does. */
  description?: string;
  /**
   * A [`Value`] describing the JSON Schema of the expected parameters.
   * Typically an object schema such as `{ "type": "object", "properties": ... }`.
   */
  parameters: any;
  /**
   * An optional [`Value`] that defines the return value schema.
   * If omitted, the tool is assumed to return free-form text or JSON.
   */
  returns?: any;
}

export interface VectorStoreAddInput {
  embedding: Embedding;
  document: string;
  metadata?: Metadata;
}

export interface VectorStoreGetResult {
  id: string;
  document: string;
  metadata?: Metadata;
  embedding: Embedding;
}

export interface VectorStoreRetrieveResult {
  id: string;
  document: string;
  metadata?: Metadata;
  distance: number;
}
