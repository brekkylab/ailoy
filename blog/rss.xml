<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Ailoy Blog</title>
        <link>https://brekkylab.github.io/ailoy/blog</link>
        <description>Ailoy Blog</description>
        <lastBuildDate>Sun, 25 May 2025 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Introducing Ailoy]]></title>
            <link>https://brekkylab.github.io/ailoy/blog/introducing-ailoy</link>
            <guid>https://brekkylab.github.io/ailoy/blog/introducing-ailoy</guid>
            <pubDate>Sun, 25 May 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[We‚Äôre excited to announce the launch of Ailoy, a drop-in library for LLM & agent development.]]></description>
            <content:encoded><![CDATA[<p>We‚Äôre excited to announce the launch of <strong>Ailoy</strong>, a drop-in library for LLM &amp; agent development.</p>
<p>Large Language Models (LLMs) and AI agents have recently emerged as a major trend in the software industry, drawing significant attention.
Yet for many developers, even building a proof of concept can be daunting.
It often requires deep knowledge of machine learning, experience with parallel programming.
Cloud APIs can ease the burden, but they introduce new cost concerns.
Taking it to a production level? That‚Äôs even harder.</p>
<p><strong>Ailoy</strong> is designed to solve these challenges.
We focused on on-device AI, which avoids complex setups and cloud costs by running entirely on the local machine.
But to access high-end AI capabilities, cloud APIs still play an important role.
Our goal is to keep the right balance between the two.
<strong>Ailoy</strong> works seamlessly across cloud, on-device, and even hybrid environments, helping you build and deploy advanced AI functionality with minimal overhead.</p>
<p>You don‚Äôt need heavy infrastructure. You don‚Äôt need a degree in AI. Just the right tools to get things done.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-what-is-ailoy">üöÄ What is Ailoy?<a href="https://brekkylab.github.io/ailoy/blog/introducing-ailoy#-what-is-ailoy" class="hash-link" aria-label="Direct link to üöÄ What is Ailoy?" title="Direct link to üöÄ What is Ailoy?">‚Äã</a></h2>
<p>Ailoy is a lightweight library that makes it easy to embed AI into your software.
Whether you're building a chatbot, an intelligent agent, or enhancing an existing system with LLM capabilities, Ailoy gives you the tools to do it with minimal effort.</p>
<p>It provides:</p>
<ul>
<li>High-level APIs to run LLMs, including multi-turn conversations, system prompt customization, and reasoning</li>
<li>Built-in tools and functions for constructing agent-based systems, including <em>Model Context Protocol (MCP)</em></li>
<li>Easy agent creation and simplified integration of <em>Retrieval-Augmented Generation (RAG)</em></li>
</ul>
<p>Think of it as your AI development toolkit.
It's ideal for embedded assistants, workflow automation, or even building your own reasoning system from the ground up.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-why-ailoy">üí° Why Ailoy?<a href="https://brekkylab.github.io/ailoy/blog/introducing-ailoy#-why-ailoy" class="hash-link" aria-label="Direct link to üí° Why Ailoy?" title="Direct link to üí° Why Ailoy?">‚Äã</a></h2>
<p><strong>1. Easy to Use</strong></p>
<p>No need to wrestle with complex orchestration frameworks or deep ML stack knowledge. With Ailoy, you can just import and go.
Ailoy is designed to feel like any modern developer library.</p>
<p><strong>2. Cloud or On-Device</strong></p>
<p>Ailoy supports both cloud vendors like OpenAI and on-device inference using optimized runtimes via TVM.
Even better, you can mix both seamlessly in a single application.</p>
<p>Want to run models offline to save on API costs? You can.
Need to fall back to the cloud when local resources aren't enough? That works too.
Or maybe you're building a hybrid that combines local tools with remote reasoning ‚Äî Ailoy makes that easy.
It's also a great fit for secure or privacy-sensitive environments where cloud access isn't an option.</p>
<p><strong>3. Versatility Across Architectures</strong></p>
<p>Ailoy supports Windows, macOS, and Linux.
And it works with Python, Node.js and C++ environment.</p>
<p>We're continuously expanding our platform and language coverage to make Ailoy available wherever developers build.</p>
<p><strong>4. Developer-Centric Design</strong></p>
<p>We built Ailoy not only for ML engineers, but also all developers.
Many existing LLM frameworks require expertise in ML systems or infrastructure.
Ailoy aims to break that barrier.
Whether you‚Äôre writing a script, building a product, or shipping a SaaS, Ailoy lets you focus on your application logic.</p>
<p>We‚Äôre currently in the early stages, but iterating rapidly to support the features and workflows real developers care about.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-what-you-can-build-with-ailoy">üîç What You Can Build with Ailoy<a href="https://brekkylab.github.io/ailoy/blog/introducing-ailoy#-what-you-can-build-with-ailoy" class="hash-link" aria-label="Direct link to üîç What You Can Build with Ailoy" title="Direct link to üîç What You Can Build with Ailoy">‚Äã</a></h2>
<ul>
<li>A multi-turn chatbot powered by a local model and cloud</li>
<li>A command-line assistant with custom tools</li>
<li>A personalized agent with access to private files via RAG</li>
<li>A desktop app that ships with an embedded LLM</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="Ô∏è-current-limitations">‚ö†Ô∏è Current Limitations<a href="https://brekkylab.github.io/ailoy/blog/introducing-ailoy#%EF%B8%8F-current-limitations" class="hash-link" aria-label="Direct link to ‚ö†Ô∏è Current Limitations" title="Direct link to ‚ö†Ô∏è Current Limitations">‚Äã</a></h2>
<p>While Ailoy strives for universality, there are a few constraints to keep in mind.</p>
<p>LLMs are still resource-intensive.
On-device execution requires a capable system, and performance can vary significantly across platforms.</p>
<p>On <code>macOS</code>, Apple silicon has made it relatively easy to run LLMs locally thanks to it's unified memory architecture and <code>Metal</code>.
However, older MacBooks may struggle to meet the performance demands.
On <code>Windows</code> and <code>Linux</code>, we currently rely on <code>Vulkan</code> for LLM execution, where GPU acceleration is often essential for achieving usable inference speeds.
As for mobile devices, we believe it's still too early for practical on-device LLM execution due to hardware limitations.
Inference speed and memory usage depend heavily on the model architecture and the capabilities of the target device.</p>
<p>That said, we‚Äôre optimistic about the future.
Many hardware vendors are now shipping computers with AI accelerators, and the landscape is quickly evolving.
We‚Äôre committed to making Ailoy compatible with these emerging platforms and enabling smooth on-device AI experiences as the hardware ecosystem matures.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-whats-next">üåê What‚Äôs Next<a href="https://brekkylab.github.io/ailoy/blog/introducing-ailoy#-whats-next" class="hash-link" aria-label="Direct link to üåê What‚Äôs Next" title="Direct link to üåê What‚Äôs Next">‚Äã</a></h2>
<p>We have a roadmap to make Ailoy even more accessible and powerful.</p>
<p>Currently, we support Alibaba‚Äôs <code>Qwen3</code> and OpenAI‚Äôs <code>GPT-4o</code>, and we plan to expand support to a broader range of models, including multimodal ones.
We're also working to enable speech recognition and text-to-speech (TTS) models, opening up more possibilities for voice-based AI interactions.
In addition, we're bringing Ailoy to the web by supporting client-side execution in modern browsers.
Language support is also expanding beyond Python and JavaScript, with Java, C#, and other ecosystems on the horizon.
We‚Äôre also planning to support inter-machine scenarios.
By building remote model execution capabilities over TCP, Ailoy will enable AI agents to communicate and collaborate across machines.
This will make it possible to build distributed agent systems that can leverage multiple machines working together.</p>
<p>Our long-term goal is to make local-first AI the default ‚Äî where on-device and cloud-based models work together seamlessly.
This hybrid approach empowers developers to build advanced AI applications while minimizing costs, protecting user privacy, and optimizing performance.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-join-us-on-the-journey">ü§ù Join Us on the Journey<a href="https://brekkylab.github.io/ailoy/blog/introducing-ailoy#-join-us-on-the-journey" class="hash-link" aria-label="Direct link to ü§ù Join Us on the Journey" title="Direct link to ü§ù Join Us on the Journey">‚Äã</a></h2>
<p>We believe accessible AI infrastructure can unlock a whole new class of applications.
Whether you're an indie developer, a startup team, or part of a large organization, Ailoy is here to help you:</p>
<ul>
<li>Streamline your development process with intuitive APIs and minimal setup</li>
<li>Build and ship on-device AI applications without worrying about high cloud costs</li>
<li>Run AI in environments where privacy, offline access, or security make cloud services impractical</li>
</ul>
<p>As we continue expanding Ailoy‚Äôs capabilities, we invite you to be part of the journey.
Your feedback, ideas, and use cases will shape what Ailoy becomes.</p>
<p><strong>Ready to build?</strong></p>
<p>Grab the package, explore the docs, and start bringing your ideas.</p>
<p><a href="https://github.com/brekkylab/ailoy" target="_blank" rel="noopener noreferrer">https://github.com/brekkylab/ailoy</a></p>]]></content:encoded>
            <category>Ailoy</category>
        </item>
    </channel>
</rss>