# RAG(Retrival-Augmented Generation) Using Documents

Retrieval-Augmented Generation (RAG) enables an AI model to use your own
documents as part of its reasoning process. Instead of relying solely on
information learned during model training, RAG allows the model to retrieve
relevant knowledge dynamically from external sources.

It leverages the AI generate more accurate, up-to-date, and context-aware
responses by grounding its answers in your document data.

In **Ailoy**, RAG is built from three pieces:

- **`EmbeddingModel`** ‚Äî turns text into vectors for similarity search.
- **`VectorStore`** ‚Äî stores and retrieves those vectors with their source text.
- **`Knowledge`** ‚Äî a runtime component that performs retrieval and feeds the
  result into the agent‚Äôs prompt.

In **preparation phase**, use the `EmbeddingModel` to encode documents and
populate the `VectorStore`. In **runtime phase**, just use the `Knowledge` to
retrieve relevant documents for each query and include them in the agent‚Äôs
context.

Plesse refer to [Architecture](../concepts/architecture#knowledge) for more
information.

## Initializing a Embedding model

<CodeTabs>

```python
import ailoy as ai

model = await ai.EmbeddingModel.new_local("BAAI/bge-m3")
```

```typescript
import * as ai from "ailoy-node";

const model = await ai.EmbeddingModel.newLocal("BAAI/bge-m3");
```

</CodeTabs>

> Ailoy currently supports both
> [**FAISS**](https://github.com/facebookresearch/faiss) and
> [**ChromaDB**](https://www.trychroma.com/) as vector store backends. Refer to
> the official configuration guide for backend-specific options.

> üí° **Note:** At this time, the only supported embedding model is
> [`BAAI/bge-m3`](https://huggingface.co/BAAI/bge-m3). Additional embedding
> models will be supported in future releases.

You can compute embeddings by calling the model‚Äôs `run()` method.

## Preparing Documents with a Vector Store

Before running RAG, create a vector store to store document embeddings for
retrieval. This step prepares your data for semantic search and may only need to
be performed once per dataset.

<CodeTabs>

```python
import ailoy as ai

CHUNKS = [
    """
He was an old man who fished alone in a skiff in the Gulf Stream and he had gone
eighty-four days now without taking a fish. In the first forty days a boy had been with him.
But after forty days without a fish the boy‚Äôs parents had told him that the old man was
now definitely and finally salao, which is the worst form of unlucky, and the boy had gone
at their orders in another boat which caught three good fish the first week. It made the
boy sad to see the old man come in each day with his skiff empty and he always went
down to help him carry either the coiled lines or the gaff and harpoon and the sail that
was furled around the mast. The sail was patched with flour sacks and, furled, it looked
like the flag of permanent defeat.
""",
"""
The old man was thin and gaunt with deep wrinkles in the back of his neck. The
brown blotches of the benevolent skin cancer the sun brings from its [9] reflection on the
tropic sea were on his cheeks. The blotches ran well down the sides of his face and his
hands had the deep-creased scars from handling heavy fish on the cords. But none of
these scars were fresh. They were as old as erosions in a fishless desert.
""",
"""
Everything about him was old except his eyes and they were the same color as the
sea and were cheerful and undefeated.
‚ÄúSantiago,‚Äù the boy said to him as they climbed the bank from where the skiff was
hauled up. ‚ÄúI could go with you again. We‚Äôve made some money.‚Äù
The old man had taught the boy to fish and the boy loved him.
‚ÄúNo,‚Äù the old man said. ‚ÄúYou‚Äôre with a lucky boat. Stay with them.‚Äù
"""
]

...

vs = ai.VectorStore.new_faiss(1024)
    chunks = {}
    for i, chunk in enumerate(CHUNKS):
        embedding = await model.run(chunk)
        id = vs.add_vector(ai.VectorStoreAddInput(embedding, chunk, {"title": "The Old Man and the Sea", "index": i}))
```

```typescript
TODO;
```

</CodeTabs>

## Defining the agent

You can now create an Agent and attach a Knowledge module that integrates your
vector store and embedding model.

:::note

To enable retrieval-based reasoning, make sure that applying the **document
polyfill** that adapts the agent‚Äôs prompt structure to include retrieved
documents.

:::

<CodeTabs>

```python
agent = ai.Agent(
    await ai.LangModel.new_local("Qwen/Qwen3-0.6B"),
    None,
    ai.Knowledge.new_vector_store(vs, model),
)
```

```typescript
TODO;
```

</CodeTabs>

## Running

To perform retrieval and generate grounded responses:

<CodeTabs>

```python
config = ai.AgentConfig(inference=ai.InferenceConfig(document_polyfill=ai.get_qwen3_polyfill()))
async for resp in agent.run("Why did the boy stop fishing with the old man?", config):
    print(resp)
```

```typescript
TODO;
```

</CodeTabs>

## Complete Example

<CodeTabs>

```python
import asyncio

import ailoy as ai

CHUNKS = [
    """
He was an old man who fished alone in a skiff in the Gulf Stream and he had gone
eighty-four days now without taking a fish. In the first forty days a boy had been with him.
But after forty days without a fish the boy‚Äôs parents had told him that the old man was
now definitely and finally salao, which is the worst form of unlucky, and the boy had gone
at their orders in another boat which caught three good fish the first week. It made the
boy sad to see the old man come in each day with his skiff empty and he always went
down to help him carry either the coiled lines or the gaff and harpoon and the sail that
was furled around the mast. The sail was patched with flour sacks and, furled, it looked
like the flag of permanent defeat.
""",
"""
The old man was thin and gaunt with deep wrinkles in the back of his neck. The
brown blotches of the benevolent skin cancer the sun brings from its [9] reflection on the
tropic sea were on his cheeks. The blotches ran well down the sides of his face and his
hands had the deep-creased scars from handling heavy fish on the cords. But none of
these scars were fresh. They were as old as erosions in a fishless desert.
""",
"""
Everything about him was old except his eyes and they were the same color as the
sea and were cheerful and undefeated.
‚ÄúSantiago,‚Äù the boy said to him as they climbed the bank from where the skiff was
hauled up. ‚ÄúI could go with you again. We‚Äôve made some money.‚Äù
The old man had taught the boy to fish and the boy loved him.
‚ÄúNo,‚Äù the old man said. ‚ÄúYou‚Äôre with a lucky boat. Stay with them.‚Äù
"""
]

async def main():
    model = await ai.EmbeddingModel.new_local("BAAI/bge-m3", print)
    vs = ai.VectorStore.new_faiss(1024)
    chunks = {}
    for i, chunk in enumerate(CHUNKS):
        embedding = await model.run(chunk)
        id = vs.add_vector(ai.VectorStoreAddInput(embedding, chunk, {"title": "The Old Man and the Sea", "index": i}))
        chunks[id] = chunk
        print(id)

    agent = ai.Agent(
        await ai.LangModel.new_local("Qwen/Qwen3-0.6B"),
        None,
        ai.Knowledge.new_vector_store(vs, model),
    )
    config = ai.AgentConfig(inference=ai.InferenceConfig(document_polyfill=ai.get_qwen3_polyfill()))
    async for resp in agent.run("Why did the boy stop fishing with the old man?", config):
        print(resp)

if __name__ == "__main__":
    asyncio.run(main())

```

```typescript
TODO;
```

</CodeTabs>

{/* prettier-ignore-start */}

:::note
For best results, ensure your documents are chunked semantically (e.g., by paragraphs or sections).
:::

{/* prettier-ignore-end */}
