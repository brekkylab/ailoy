# RAG Using Documents

Retrieval-Augmented Generation (RAG) enables an AI model to use your own
documents as part of its reasoning process. Instead of relying solely on
information learned during model training, RAG allows the model to retrieve
relevant knowledge dynamically from external sources.

It enables the AI generate more accurate, up-to-date, and context-aware
responses by grounding its answers in your document data.

In **Ailoy**, RAG is built from three pieces:

- **`EmbeddingModel`** â€” turns text into vectors for similarity search.
- **`VectorStore`** â€” stores and retrieves those vectors with their source text.
- **`Knowledge`** â€” a runtime component that performs retrieval and feeds the
  result into the agentâ€™s prompt.

In the **preparation phase**, use the `EmbeddingModel` to encode documents and
populate the `VectorStore`. In the **runtime phase**, just use the `Knowledge`
to retrieve relevant documents for each query and include them in the agentâ€™s
context.

Please refer to [Architecture](../concepts/architecture#knowledge) section for
more information.

## Step-by-Step Guide

### Initializing an Embedding Model

<CodeTabs>

```python
import ailoy as ai

async def prepare_knowledge():
    model = await ai.EmbeddingModel.new_local("BAAI/bge-m3")

if __name__ == "__main__":
    asyncio.run(prepare_knowledge())
```

```typescript
import * as ai from "ailoy-node";

async function prepare_knowledge() {
  const model = await ai.EmbeddingModel.newLocal("BAAI/bge-m3");
}

prepare_knowledge().catch((err) => {
  console.error("Error:", err);
});
```

```typescript web
import * as ai from "ailoy-web";

async function prepare_knowledge() {
  const model = await ai.EmbeddingModel.newLocal("BAAI/bge-m3");
}

prepare_knowledge().catch((err) => {
  console.error("Error:", err);
});
```

</CodeTabs>

:::note

ðŸ’¡ **Note:** At this time, the only supported embedding model is
[`BAAI/bge-m3`](https://huggingface.co/BAAI/bge-m3). Additional embedding models
will be supported in future releases.

:::

### Preparing Documents with a Vector Store

Before running RAG, create a vector store to store document embeddings for
retrieval. This step prepares your data for semantic search and may only need to
be performed once per dataset.

<CodeTabs>

```python
import asyncio

import ailoy as ai

CHUNKS = [...]

async def prepare_knowledge() -> ai.Knowledge:
    model = await ai.EmbeddingModel.new_local("BAAI/bge-m3")
    vs = ai.VectorStore.new_faiss(1024)
    for i, chunk in enumerate(CHUNKS):
        embedding = await model.infer(chunk)
        vs.add_vector(
            ai.VectorStoreAddInput(
                embedding,
                chunk,
                {"title": "The Old Man and the Sea", "index": i}
            )
        )

    return ai.Knowledge.new_vector_store(vs, model)

if __name__ == "__main__":
    asyncio.run(prepare_knowledge())
```

```typescript
import * as ai from "ailoy-node";

const CHUNKS = [...];

async function prepare_knowledge() {
  const model = await ai.EmbeddingModel.newLocal("BAAI/bge-m3");
  const vs = await ai.VectorStore.newFaiss(1024);
  CHUNKS.forEach(async (chunk, i) => {
      const embedding = await model.infer(chunk);
      await vs.addVector({
          embedding: embedding,
          document: chunk,
          metadata: {
              title: "The Old Man and the Sea",
              index: i,
          },
      });
  });

  return ai.Knowledge.newVectorStore(vs, model);
}

prepare_knowledge().catch((err) => {
  console.error("Error:", err);
});
```

```typescript web
import * as ai from "ailoy-web";

const CHUNKS = [...];

async function prepare_knowledge() {
  const model = await ai.EmbeddingModel.newLocal("BAAI/bge-m3");
  const vs = await ai.VectorStore.newFaiss(1024);
  CHUNKS.forEach(async (chunk, i) => {
      const embedding = await model.infer(chunk);
      await vs.addVector({
          embedding: embedding,
          document: chunk,
          metadata: {
              title: "The Old Man and the Sea",
              index: i,
          },
      });
  });

  return ai.Knowledge.newVectorStore(vs, model);
}

prepare_knowledge().catch((err) => {
  console.error("Error:", err);
});
```

</CodeTabs>

> Ailoy currently supports both
> [**FAISS**](https://github.com/facebookresearch/faiss) and
> [**ChromaDB**](https://www.trychroma.com/) as vector store backends. Refer to
> the official configuration guide for backend-specific options.

### Defining the Agent

You can now create an Agent and attach a Knowledge module that integrates your
vector store and embedding model.

:::note

To enable retrieval-based reasoning, make sure that applying the **document
polyfill** that adapts the agentâ€™s prompt structure to include retrieved
documents.

:::

<CodeTabs>

```python
import asyncio

import ailoy as ai


async def prepare_knowledge():
    ...


async def main(knowledge: ai.Knowledge):
  agent = ai.Agent(
      await ai.LangModel.new_local("Qwen/Qwen3-0.6B"),
      knowledge=knowledge,
  )

if __name__ == "__main__":
    knowledge = asyncio.run(prepare_knowledge())
    asyncio.run(main(knowledge))
```

```typescript
import * as ai from "ailoy-node";

async function prepare_knowledge() {...}

async function main(knowledge: ai.Knowledge) {
  const agent = new ai.Agent(
    await ai.LangModel.newLocal("Qwen/Qwen3-0.6B"),
    undefined /* tools */,
    knowledge
  );
}

prepare_knowledge()
  .then((knowledge) => main(knowledge))
  .catch((err) => {
    console.error("Error:", err);
  });
```

```typescript web
import * as ai from "ailoy-web";

async function prepare_knowledge() {...}

async function main(knowledge: ai.Knowledge) {
  const agent = new ai.Agent(
    await ai.LangModel.newLocal("Qwen/Qwen3-0.6B"),
    undefined /* tools */,
    knowledge
  );
}

prepare_knowledge()
  .then((knowledge) => main(knowledge))
  .catch((err) => {
    console.error("Error:", err);
  });
```

</CodeTabs>

### Performing RAG

To perform retrieval and generate grounded responses:

<CodeTabs>

```python
import asyncio

import ailoy as ai


async def prepare_knowledge():
    ...


async def main(knowledge: ai.Knowledge):
    agent = ai.Agent(
        await ai.LangModel.new_local("Qwen/Qwen3-0.6B"),
        knowledge=knowledge,
    )
    config = ai.AgentConfig(
        # Qwen3 doesn't support documents directly, we need to add the document polyfill in config.
        inference=ai.InferenceConfig(document_polyfill=ai.get_qwen3_polyfill())
    )
    async for resp in agent.run("Why did the boy stop fishing with the old man?", config):
        print(resp.message.contents[0].text)

if __name__ == "__main__":
    knowledge = asyncio.run(prepare_knowledge())
    asyncio.run(main(knowledge))
```

```typescript
import * as ai from "ailoy-node";

async function prepare_knowledge() {...}

async function main(knowledge: ai.Knowledge) {
  const agent = new ai.Agent(
    await ai.LangModel.newLocal("Qwen/Qwen3-0.6B"),
    undefined /* tools */,
    knowledge
  );
  const config = { inference: { documentPolyfill: ai.getQwen3Polyfill() } };
  for await (const resp of agent.run(
    "Why did the boy stop fishing with the old man?",
    config
  )) {
    console.log(resp.message.contents[0].text);
  }
}

prepare_knowledge()
  .then((knowledge) => main(knowledge))
  .catch((err) => {
    console.error("Error:", err);
  });
```

```typescript web
import * as ai from "ailoy-web";

async function prepare_knowledge() {...}

async function main(knowledge: ai.Knowledge) {
  const agent = new ai.Agent(
    await ai.LangModel.newLocal("Qwen/Qwen3-0.6B"),
    undefined /* tools */,
    knowledge
  );
  const config = { inference: { documentPolyfill: ai.getQwen3Polyfill() } };
  for await (const resp of agent.run(
    "Why did the boy stop fishing with the old man?",
    config
  )) {
    console.log(resp.message.contents[0].text);
  }
}

prepare_knowledge()
  .then((knowledge) => main(knowledge))
  .catch((err) => {
    console.error("Error:", err);
  });
```

</CodeTabs>

## Complete Example

<CodeTabs>

```python
import asyncio

import ailoy as ai

CHUNKS = [
    """
He was an old man who fished alone in a skiff in the Gulf Stream and he had gone
eighty-four days now without taking a fish. In the first forty days a boy had been with him.
But after forty days without a fish the boyâ€™s parents had told him that the old man was
now definitely and finally salao, which is the worst form of unlucky, and the boy had gone
at their orders in another boat which caught three good fish the first week. It made the
boy sad to see the old man come in each day with his skiff empty and he always went
down to help him carry either the coiled lines or the gaff and harpoon and the sail that
was furled around the mast. The sail was patched with flour sacks and, furled, it looked
like the flag of permanent defeat.
""",
    """
The old man was thin and gaunt with deep wrinkles in the back of his neck. The
brown blotches of the benevolent skin cancer the sun brings from its [9] reflection on the
tropic sea were on his cheeks. The blotches ran well down the sides of his face and his
hands had the deep-creased scars from handling heavy fish on the cords. But none of
these scars were fresh. They were as old as erosions in a fishless desert.
""",
    """
Everything about him was old except his eyes and they were the same color as the
sea and were cheerful and undefeated.
â€œSantiago,â€ the boy said to him as they climbed the bank from where the skiff was
hauled up. â€œI could go with you again. Weâ€™ve made some money.â€
The old man had taught the boy to fish and the boy loved him.
â€œNo,â€ the old man said. â€œYouâ€™re with a lucky boat. Stay with them.â€
""",
]

async def prepare_knowledge():
    model = await ai.EmbeddingModel.new_local("BAAI/bge-m3")
    vs = ai.VectorStore.new_faiss(1024)
    for i, chunk in enumerate(CHUNKS):
        embedding = await model.run(chunk)
        vs.add_vector(
            ai.VectorStoreAddInput(
                embedding, chunk, {"title": "The Old Man and the Sea", "index": i}
            )
        )

    return ai.Knowledge.new_vector_store(vs, model)


async def main(knowledge: ai.Knowledge):
    agent = ai.Agent(
        await ai.LangModel.new_local("Qwen/Qwen3-0.6B"),
        knowledge=knowledge,
    )
    config = ai.AgentConfig(
        inference=ai.InferenceConfig(document_polyfill=ai.get_qwen3_polyfill())
    )
    async for resp in agent.run("Why did the boy stop fishing with the old man?", config):
        print(resp.message.contents[0].text)


if __name__ == "__main__":
    knowledge = asyncio.run(prepare_knowledge())
    asyncio.run(main(knowledge))
```

```typescript
import * as ai from "ailoy-node";

const CHUNKS = [
  `
He was an old man who fished alone in a skiff in the Gulf Stream and he had gone
eighty-four days now without taking a fish. In the first forty days a boy had been with him.
But after forty days without a fish the boyâ€™s parents had told him that the old man was
now definitely and finally salao, which is the worst form of unlucky, and the boy had gone
at their orders in another boat which caught three good fish the first week. It made the
boy sad to see the old man come in each day with his skiff empty and he always went
down to help him carry either the coiled lines or the gaff and harpoon and the sail that
was furled around the mast. The sail was patched with flour sacks and, furled, it looked
like the flag of permanent defeat.
`,
  `
The old man was thin and gaunt with deep wrinkles in the back of his neck. The
brown blotches of the benevolent skin cancer the sun brings from its [9] reflection on the
tropic sea were on his cheeks. The blotches ran well down the sides of his face and his
hands had the deep-creased scars from handling heavy fish on the cords. But none of
these scars were fresh. They were as old as erosions in a fishless desert.
`,
  `
Everything about him was old except his eyes and they were the same color as the
sea and were cheerful and undefeated.
â€œSantiago,â€ the boy said to him as they climbed the bank from where the skiff was
hauled up. â€œI could go with you again. Weâ€™ve made some money.â€
The old man had taught the boy to fish and the boy loved him.
â€œNo,â€ the old man said. â€œYouâ€™re with a lucky boat. Stay with them.â€
`,
];

async function prepare_knowledge() {
  const model = await ai.EmbeddingModel.newLocal("BAAI/bge-m3");
  const vs = await ai.VectorStore.newFaiss(1024);
  CHUNKS.forEach(async (chunk, i) => {
    const embedding = await model.infer(chunk);
    await vs.addVector({
      embedding: embedding,
      document: chunk,
      metadata: {
        title: "The Old Man and the Sea",
        index: i,
      },
    });
  });

  return ai.Knowledge.newVectorStore(vs, model);
}

async function main(knowledge: ai.Knowledge) {
  const agent = new ai.Agent(
    await ai.LangModel.newLocal("Qwen/Qwen3-0.6B"),
    undefined,
    knowledge
  );

  const config = { inference: { documentPolyfill: ai.getQwen3Polyfill() } };
  for await (const resp of agent.run(
    "Why did the boy stop fishing with the old man?",
    config
  )) {
    if (resp.message.contents?.[0]?.type === "text")
      console.log(resp.message.contents?.[0]?.text);
  }
}

prepare_knowledge()
  .then((knowledge) => main(knowledge))
  .catch((err) => {
    console.error("Error:", err);
  });
```

```typescript web
import * as ai from "ailoy-web";

const CHUNKS = [
  `
He was an old man who fished alone in a skiff in the Gulf Stream and he had gone
eighty-four days now without taking a fish. In the first forty days a boy had been with him.
But after forty days without a fish the boyâ€™s parents had told him that the old man was
now definitely and finally salao, which is the worst form of unlucky, and the boy had gone
at their orders in another boat which caught three good fish the first week. It made the
boy sad to see the old man come in each day with his skiff empty and he always went
down to help him carry either the coiled lines or the gaff and harpoon and the sail that
was furled around the mast. The sail was patched with flour sacks and, furled, it looked
like the flag of permanent defeat.
`,
  `
The old man was thin and gaunt with deep wrinkles in the back of his neck. The
brown blotches of the benevolent skin cancer the sun brings from its [9] reflection on the
tropic sea were on his cheeks. The blotches ran well down the sides of his face and his
hands had the deep-creased scars from handling heavy fish on the cords. But none of
these scars were fresh. They were as old as erosions in a fishless desert.
`,
  `
Everything about him was old except his eyes and they were the same color as the
sea and were cheerful and undefeated.
â€œSantiago,â€ the boy said to him as they climbed the bank from where the skiff was
hauled up. â€œI could go with you again. Weâ€™ve made some money.â€
The old man had taught the boy to fish and the boy loved him.
â€œNo,â€ the old man said. â€œYouâ€™re with a lucky boat. Stay with them.â€
`,
];

async function prepare_knowledge() {
  const model = await ai.EmbeddingModel.newLocal("BAAI/bge-m3");
  const vs = await ai.VectorStore.newFaiss(1024);
  CHUNKS.forEach(async (chunk, i) => {
    const embedding = await model.infer(chunk);
    await vs.addVector({
      embedding: embedding,
      document: chunk,
      metadata: {
        title: "The Old Man and the Sea",
        index: i,
      },
    });
  });

  return ai.Knowledge.newVectorStore(vs, model);
}

async function main(knowledge: ai.Knowledge) {
  const agent = new ai.Agent(
    await ai.LangModel.newLocal("Qwen/Qwen3-0.6B"),
    undefined,
    knowledge
  );

  const config = { inference: { documentPolyfill: ai.getQwen3Polyfill() } };
  for await (const resp of agent.run(
    "Why did the boy stop fishing with the old man?",
    config
  )) {
    if (resp.message.contents?.[0]?.type === "text")
      console.log(resp.message.contents?.[0]?.text);
  }
}

prepare_knowledge()
  .then((knowledge) => main(knowledge))
  .catch((err) => {
    console.error("Error:", err);
  });
```

</CodeTabs>

{/* prettier-ignore-start */}

:::note
For best results, ensure your documents are chunked semantically (e.g., by paragraphs or sections).
:::

## Output

<TerminalBox>

The boy stopped fishing with the old man because he was with a lucky boat, not because he was in possession of a boat or because he was with the old man. The boy was with a boat, not a person. The old man had taught the boy to fish, and the boy loved him, but the reason for stopping fishing was related to the boat.

</TerminalBox>

{/* prettier-ignore-end */}
