# Getting started

**_Welcome to Ailoy‚Äôs Tutorial!_** ü§ó

In this tutorial, we‚Äôll explore how to run LLMs in Ailoy and extend their
capabilities to build agent systems.

## Installation

You can install Ailoy via a package manager. Just type the following command in
your shell:

{/* prettier-ignore-start */}

<CodeTabs>

```python
pip install ailoy-py
```

```typescript
// Using npm
npm install ailoy-node
// Using yarn
yarn add ailoy-node
```

```typescript web
// Using npm
npm install ailoy-web
// Using yarn
yarn add ailoy-web
```

</CodeTabs>

{/* prettier-ignore-end */}

{/* prettier-ignore-start */}

:::info
For using Ailoy in web browsers, see [WebAssembly Supports](./webassembly-supports.mdx) for more details.
:::

{/* prettier-ignore-end */}

## Running a Simplest Agent

The **Language Model (LM)** is the core of every agent.  
An agent system operates based on a language model and its instructions, along
with various extensions that enhance its capabilities.

### Choosing Between API and Local Models

Ailoy lets you run language model in two different modes:

- **API Models** ‚Äì run models hosted by AI vendors (like OpenAI GPT or Anthropic
  Claude)
- **Local Models** ‚Äì run open-source models directly on your own machine

Each approach has its own strengths and trade-offs:

- **API Models**: These connect to external AI services over the internet. If
  you‚Äôre fine with API token costs and prefer cloud-hosted models, this is the
  simplest option.
- **Local Models**: These run entirely on your own device, using open-source
  models. Choose this if you want full control, offline availability, or
  privacy.

Ailoy supports both seamlessly ‚Äî you can switch between them anytime with
minimal changes.

## Example

Let‚Äôs start with a simple example ‚Äî Ailoy‚Äôs version of ‚ÄúHello, World!‚Äù for LLMs.

In just a few lines of code, you‚Äôll run a model, send it a prompt, and print the
response.

### Using API models

{/* prettier-ignore-start */}

<CodeTabs>

```python
import asyncio

import ailoy as ai


async def main():
    lm = ai.LangModel.new_stream_api(
        spec="OpenAI", 
        model_name="gpt-4o", 
        api_key="YOUR_OPENAI_API_KEY"
    )
    agent = ai.Agent(lm)
    async for resp in agent.run("Please give me a short poem about AI."):
        if isinstance(resp.message.contents[0], ai.Part.Text):
            print(resp.message.contents[0].text)


if __name__ == "__main__":
    asyncio.run(main())
```

```typescript
import * as ai from "ailoy-node";

async function main() {
  const lm = await ai.LangModel.newStreamAPI(
    "OpenAI",              // spec
    "gpt-5",               // modelName
    "YOUR_OPENAI_API_KEY", // apiKey
  );
  const agent = new ai.Agent(lm);
  for await (const resp of agent.run("Please give me a short poem about AI")) {
    if (resp.message.contents[0].type === "text") {
      console.log(resp.message.contents[0].text); 
    }
  }
}

main().catch((err) => {
  console.error("Error:", err);
});
```

```typescript web
import * as ai from "ailoy-web";

async function main() {
  const lm = await ai.LangModel.newStreamAPI(
    "OpenAI",              // spec
    "gpt-5",               // modelName
    "YOUR_OPENAI_API_KEY", // apiKey
  );
  const agent = new ai.Agent(lm);
  for await (const resp of agent.run("Please give me a short poem about AI")) {
    if (resp.message.contents[0].type === "text") {
      console.log(resp.message.contents[0].text); 
    }
  }
}

main().catch((err) => {
  console.error("Error:", err);
});
```

</CodeTabs>

{/* prettier-ignore-end */}

### Using local models

Here, we use the open-source model `Qwen3-0.6B`, running entirely on your own
device ‚Äî no API keys or internet connection required.

{/* prettier-ignore-start */}

<CodeTabs>

```python
import asyncio

import ailoy as ai


async def main():
    lm = await ai.LangModel.new_local(
        model_name="Qwen/Qwen3-0.6B", progress_callback=print
    )
    agent = ai.Agent(lm)
    async for resp in agent.run("Please give me a short poem about AI."):
        if isinstance(resp.message.contents[0], ai.Part.Text):
            print(resp.message.contents[0].text)


if __name__ == "__main__":
    asyncio.run(main())

```

```typescript
import * as ai from "ailoy-node";

async function main() {
  const lm = await ai.LangModel.newLocal(
    "Qwen/Qwen3-0.6B", // modelName
    console.log,       // progressCallback?
  );
  const agent = new ai.Agent(lm);
  for await (const resp of agent.run("Please give me a short poem about AI")) {
    if (resp.message.contents[0].type === "text") {
      console.log(resp.message.contents[0].text); 
    }
  }
}

main().catch((err) => {
  console.error("Error:", err);
});
```

```typescript web
import * as ai from "ailoy-web";

async function main() {
  const lm = await ai.LangModel.newLocal(
    "Qwen/Qwen3-0.6B", // modelName
    console.log,       // progressCallback?
  );
  const agent = new ai.Agent(lm);
  for await (const resp of agent.run("Please give me a short poem about AI")) {
    if (resp.message.contents[0].type === "text") {
      console.log(resp.message.contents[0].text); 
    }
  }
}

main().catch((err) => {
  console.error("Error:", err);
});
```

</CodeTabs>

{/* prettier-ignore-end */}

{/* prettier-ignore-start */}

:::info
You can use the Ailoy
[CLI interface](../concepts/command-line-interfaces) to manage downloaded model
files. 
:::

{/* prettier-ignore-end */}

### Output

Since the model needs to be downloaded and initialized, the first run may take a
little time.  
After a short wait, you may see an output similar to this.

<TerminalBox>

In the digital realm, where thoughts run,  
AI dreams, no more than dreams of our own.  
With code and data, it creates, it learns,  
But dreams, still, run in hearts, in minds.

</TerminalBox>

That‚Äôs it! üéâ  
You‚Äôve just run your first AI model with Ailoy.

{/* prettier-ignore-start */}

:::info 
For a detailed explanation of the input and output formats in Ailoy, please refer to [Chat Completion Format](../concepts/chat-completion-format).
:::

:::info 
Don't be surprised if the output changes each time you run it.

An LLM's output includes a certain level of randomness, controlled by the `temperature` and `top_p`.
In ailoy, it can be adjusted with `InferenceConfig` in `AgentConfig`.

If both `temperature` and `top_p` are set to 0, the answer becomes deterministic.
:::

{/* prettier-ignore-end */}
