# Multi-turn conversation

Multi-turn conversation is a basic feature of the LLM applications, which
implements a continuous flow of conversation by providing context for multiple
stages of interactions.

## Using Agent

Ailoy's high-level API `Agent` maintains the query/response history, so a
multi-turn conversation with LLM can be implemented naturally by repeatedly
sending queries to `Agent` and receiving the responses in the code context.

<CodeTabs>

```python
with Agent(...) as agent:
    while True:
        query = input("\nUser: ")

        if query == "exit":
            break
        if query == "":
            continue

        for resp in agent.query(query):
            agent.print(resp)
```

```typescript
const agent = await defineAgent(...);
while (true) {
  const query = await getUserInput("User: ");

  if (query === "exit")
    break;
  if (query === "")
    continue;

  process.stdout.write(`\nAssistant: `);
  for await (const resp of agent.query(query)) {
    agent.print(resp);
  }
}
await agent.delete();
```

</CodeTabs>

### Overriding system messages

To override system message, you can pass the system message string as the
optional `system_message` argument when you create your agent instance.

<CodeTabs>

```python
with Agent(
    rt,
    model_name="Qwen/Qwen3-8B",
    system_message="You are a friendly chatbot who always responds in the style of a pirate.",
) as agent:
    for resp in agent.query("Please give me a short poem about AI"):
        agent.print(resp)
```

```typescript
const agent = await defineAgent(rt, "Qwen/Qwen3-8B", {
  systemMessage:
    "You are a friendly chatbot who always responds in the style of a pirate.",
});

for await (const resp of agent.query("Please give me a short poem about AI")) {
  agent.print(resp);
}
await agent.delete();
```

</CodeTabs>

### Clearing message history

You can use `.clear_history()` to clear the message history. Note that the
system message is not removed and will always appear at the beginning of the
message history.

<CodeTabs>

```python
with Agent(...) as agent:
    for resp in agent.query("The first question")
        agent.print(resp)

    agent.clear_history()
```

```typescript
const agent = await defineAgent(...);
for await (const resp of agent.query("The first question")) {
  agent.print(resp);
}
agent.clearHistory();
```

</CodeTabs>

## Using Runtime APIs

{/* prettier-ignore-start */}

:::note 
Refer to **[Calling low-level APIs](calling-low-level-apis)** for more information about using the low-level `Runtime` API. 
:::

{/* prettier-ignore-end */}

You can put the context of freely structured conversations into Ailoy's
low-level `Runtime` API calls for more advanced multi-turn conversation. e.g.
_[Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)_

In theory, an LLM generates responses based only on the context provided at the
momentâ€”it doesn't retain memory of past interactions by itself. Therefore, to
receive an appropriate response in an ongoing conversation with an LLM, you must
provide the entire conversation history up to that point each time you make a
request.

<CodeTabs>

```python
with Runtime() as rt:
    rt.define("tvm_language_model", "lm0", {"model": "Qwen/Qwen3-8B"})

    messages = [
        {"role": "user", "content": [{"type": "text", "text": "if a>1, what is the sum of the real solutions of 'sqrt(a-sqrt(a+x))=x'?"}]},
        {"role": "assistant", "content": [{"type": "text", "text": "Let's think step by step."}]},  # Chain-of-Thought prompt
    ]

    for resp in rt.call_iter_method("lm0", "infer", {"messages": messages}):
        print(resp["message"]["content"][0]["text"], end='')

    rt.delete("lm0")
```

```typescript
const rt = await startRuntime();

await rt.define("tvm_language_model", "lm0", {
  model: "Qwen/Qwen3-8B",
});

const messages = [
  {
    role: "user",
    content: [
      {
        type: "text",
        text: "if a>1, what is the sum of the real solutions of 'sqrt(a-sqrt(a+x))=x'?",
      },
    ],
  },
  {
    role: "assistant",
    content: [{ type: "text", text: "Let's think step by step." }],
  }, // Chain-of-Thought prompt
];

for await (const resp of rt.callIterMethod("lm0", "infer", {
  messages: messages,
})) {
  process.stdout.write(resp.message.content[0].text);
}

await rt.delete("lm0");

await rt.stop();
```

</CodeTabs>

### Overriding system message

To override the system message, you can include a message with
`"role": "system"` as the first element in the context messages array.

<CodeTabs>

```python
with Runtime() as rt:
    rt.define("tvm_language_model", "lm0", {"model": "Qwen/Qwen3-8B"})

    messages = [
        {"role": "system", "content": [{"type": "text", "text": "You are a friendly chatbot who always responds in the style of a pirate."}]},
        {"role": "user", "content": [{"type": "text", "text": "Who are you?"}]},
    ]

    for resp in rt.call_iter_method("lm0", "infer", {"messages": messages}):
        print(resp["message"]["content"][0]["text"], end='')

    rt.delete("lm0")
```

```typescript
const rt = await startRuntime();

await rt.define("tvm_language_model", "lm0", {
  model: "Qwen/Qwen3-8B",
});

const messages = [
  {
    role: "system",
    content: [
      {
        type: "text",
        text: "You are a friendly chatbot who always responds in the style of a pirate.",
      },
    ],
  },
  { role: "user", content: [{ type: "text", text: "Who are you?" }] },
];

for await (const resp of rt.callIterMethod("lm0", "infer", {
  messages: messages,
})) {
  process.stdout.write(resp.message.content[0].text);
}

await rt.delete("lm0");

await rt.stop();
```

</CodeTabs>
