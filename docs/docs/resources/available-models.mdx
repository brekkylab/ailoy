# Available Models

Currently, the following AI models are supported:

{/* prettier-ignore-start */}
## Language Models

### Local Models
- <SmallIcon
    src="https://assets.alicdn.com/g/qwenweb/qwen-webui-fe/0.0.239/static/favicon.png"
  />
  Qwen3
  - `Qwen/Qwen3-0.6B`
  - `Qwen/Qwen3-1.7B`
  - `Qwen/Qwen3-4B`
  - `Qwen/Qwen3-8B`
  - `Qwen/Qwen3-14B`
  - `Qwen/Qwen3-32B`
  - `Qwen/Qwen3-30B-A3B` (MoE)

### API Models
- <SmallIcon
    src="https://openai.com/favicon.svg"
  />
  OpenAI
- <SmallIcon
    src="https://gemini.google/images/spark_4c.png"
  />
  Gemini
- <SmallIcon
    src="https://claude.ai/favicon.ico"
  />
  Claude
- <SmallIcon
    src="https://console.x.ai/_next/static/media/favicon.20ac9181.ico"
  />
  Grok

## Embedding Models

### Local Models
- <SmallIcon
    src="https://registry.npmmirror.com/@lobehub/icons-static-png/latest/files/dark/baai.png"
  />
  BAAI/bge-m3

{/* prettier-ignore-end */}

### VRAM requirements

{/* prettier-ignore-start */}

:::warning These values may vary depending on the environment and circumstances.
:::

{/* prettier-ignore-end */}

Requirements for available VRAM size by models are estimated as follows:

| Model                | Context length | VRAM (params) | VRAM (total) |
| -------------------- | :------------: | :-----------: | :----------: |
| `BAAI/bge-m3`        |       8k       |   ≈ 0.3 GB    |   ≈ 0.3 GB   |
| `Qwen/Qwen3-0.6B`    |      40k       |   ≈ 0.5 GB    |   ≈ 5.0 GB   |
| `Qwen/Qwen3-1.7B`    |      40k       |   ≈ 1.0 GB    |   ≈ 5.5 GB   |
| `Qwen/Qwen3-4B`      |      40k       |   ≈ 2.4 GB    |   ≈ 8.0 GB   |
| `Qwen/Qwen3-8B`      |      40k       |   ≈ 4.5 GB    |  ≈ 10.5 GB   |
| `Qwen/Qwen3-14B`     |      40k       |   ≈ 8.0 GB    |  ≈ 14.5 GB   |
| `Qwen/Qwen3-32B`     |      40k       |   ≈ 17.6 GB   |   ≈ 25 GB    |
| `Qwen/Qwen3-30B-A3B` |      40k       |   ≈ 16.5 GB   |   ≈ 24 GB    |
