import AgentComponentsSvg from "./img/agent-components.svg";
import ToolStructureSvg from "./img/tool-structure.svg";

# How Agent Works

<p align="center">
  <AgentComponentsSvg style={{ width: "60%", height: "60%" }} />
</p>

The Agent is composed of a **language model** and various components like **tools** and **knowledge**, augmenting the functionality of the language model.
Each component communicates with one another through a unified format called **message**.

## Language Model

A **language model(LM)** is a type of AI system that can understand and generate natural language.
You can think of it as a highly advanced autocomplete engine.
It predicts what words or sentences should come next based on the context it has already seen.
Trained on massive datasets, it can hold meaningful conversations, write code, summarize documents, and even reason about complex problems.
Yet, in a way, it’s a simple system at its core.

To use Ailoy effectively, it helps to have a clear understanding of how a language model works.
In this section, we’ll briefly explain the process how an LM operates.

### Step 1: Tokenization

At the most basic level, a language model working with **tokens**.
It inputs tokens and generates next token.

:::note
Ailoy uses a special text structure called the **chat completion format**.
Please refer to [] about this format.
:::

As mentioned above, input text is broken into tokens.
Each token corresponds to a whole word, a subword, or even a single character, depending on the model’s vocabulary.
Each token is then mapped to a number (its ID), forming a numerical sequence that the model can process.

For example:

```
Text: "The sky is blue today."
Tokens (string): ["The", " sky", " is", " blue", "today", "."]
Tokens (IDs): [172,  10400,  8172,  404, 629, 100]
```

### Step 2: Inference

Once the input sequence is encoded, the model predicts next token.
It estimates a probability distribution over all possible next tokens.
In simple terms, it answers the question:

> “Given what I’ve seen so far, what is the most likely next token?”

For example:

- Input: "The sky is"
- Prediction:
  - "blue" → 0.92  
  - "green" → 0.03  
  - "red" → 0.01
  - ...

The model then chooses one token based on these probabilities.
It might take the most likely one (for accurate results) or sample randomly among several (for more creative outputs).

### Step 3: Iterative Generation

After generating a token, the model appends it to the input sequence and repeats the same process again.
It re-encodes the new sequence, computes attention, predicts the next token, and continues this loop.

For example:

- Input: "The sky is" → Model predicts "blue"
- New input: "The sky is blue" → Model predicts "today"
- New input: "The sky is blue today" → Model predicts "." (period)

Then it emits a stop token and finishes.

This iterative process continues until the model decides the text is complete or reaches a set limit.
The result is a coherent and contextually consistent piece of text.

## Message

Many language models are fine-tuned for chat-based interactions, enabling them to engage in natural conversations with users.
Through the conversation format, the model can understand a user’s natural-language query and generate an appropriate, context-aware response.
This process often referred to as **chat completion**.

See the [chat completion format](./chat-completion-format) for more information.

## How Tool Calling Works

In most agent system, tool calling can be achieved by the following process.

<p align="center">
  <ToolStructureSvg style={{ width: "40%", height: "40%" }} />
</p>

1. **\[Tool Description\]** Assistant (or LLM) can recognize a tool based on its
   description.
2. **\[User Query\]** User provides an input prompt to the assistant.
3. **\[Tool Call\]** If that query is related to a tool, assistant can invoke
   the tool through a specified format.
4. **\[Tool Result\]** Tool performs its task as invoked and returns the result.
5. **\[Assistant Response\]** Assistant can incorporate the tool's output to
   produce a more accurate answer.

## How RAG Works

TBD
