# Chat Completion Format

Many language models are fine-tuned for chat-based interactions, enabling them to engage in natural conversations with users.
Through the conversation format, the model can understand a user’s natural-language query and generate an appropriate, context-aware response.
This process often referred to as **chat completion**.

Conventionally, in a chat completion setup, both **input** and **output** follow a structured format.
Messages are typically represented as follows:

```
[
  {
    "role": "system",
    "contents": [
      { "type": "text", "text": "You are a friendly and knowledgeable assistant." }
    ]
  },
  {
    "role": "user",
    "contents": [
      { "type": "text", "text": "Can you explain how photosynthesis works?" }
    ]
  }
]
```

When it is executed, the output might look like:
```
{
    "role": "assistant",
    "contents": [
        {
            "type": "text",
            "text": "Photosynthesis is the process by which plants convert sunlight, water, and carbon dioxide into energy. They use sunlight to produce glucose (a form of sugar) and release oxygen as a byproduct."
        }
    ]
}
```

## Message

A `Message` represents one conversational turn — what one participant (system, user, or assistant) says or does.

Each message contains:
- a **role**, indicating who is speaking,
- a set of **contents**, describing what was said or sent.

It associated with a specific role and a content.

### Role

|Role|Description|
|---|---|
|**System**|System instructions and constraints provided to the assistant. It usually defines the model’s behavior or persona.|
|**User**|Contents authored by the user.|
|**Assistant**|Contents automatically generated by the assistant/AI model.|
|**Tool**|Execution results produced by external tools/functions.|

### Contents

What is said from each role is generally referred to as content.
However, a model can operate in different modes, producing outputs that serve different purposes.

These outputs reflect the **intention** behind what the model says or performs.

|Intent|Description|
|---|---|
|**Content**|General output.|
|**Reasoning(thinking)**|Some model generates intermediate thoughts or reasoning traces before producing the final answer. These are stored in the reasoning field (often hidden from the user) and can help trace or visualize the model’s internal decision process.|
|**Tool call**|When the model decides to invoke an external function or API instead of generating plain text. These are represented as structured objects that describe which function to call(name) and with what arguments.|

These types of outputs are stored in separate fields within a message, making it possible to distinguish from general conversation.

**Example**

```
[
  Message {
      role: "assistant",
      reasoning: "Let’s reason step by step: photosynthesis converts light energy into chemical energy...",
      contents: [
          { type: "text", text: "Photosynthesis is the process by which plants convert sunlight, water, and carbon dioxide into energy." }
      ],
      tool_calls: [
          {
            type: "function",
            function: {
              id: "func_call_1234abcd",
              name: "get_current_location",
              arguments: ...
            }
          }
      ],
  },
]
```

## Parts

While `Contents` describe the intention of a message, `Part` defines the data type of that message.

`Part` can be considered as a smallest semantic units within a conversation.
Each `Message` contains a list of `Part` objects.
A `Part` can represent text, images, function calls, or any structured values, enabling rich multimodal communication.

|Part|Description|
|---|---|
|**Text**|Natural-language text content|
|**Image**|Visual data or reference (e.g., binary, URL, or metadata)|
|**Function**|Structured tool or function invocation|
|**Value**|Arbitrary data (numbers, objects, JSON values, etc.)|

For example, if a user asks about an image, the message could look like this:

```
contents: [
  {type: "image", "image": {data: "..."}}
  {type: "text", text: "What you can see in this image?"} 
]
```

Together, these parts express Ailoy’s multimodal conversation.

## Delta

The inference of a language model can take a significant amount of time.
To improve real-time responsiveness, many AI systems stream tokens as they are generated.
These streamed outputs are typically delivered in the form of **deltas**.

A **delta**(`MessageDelta` or `PartDelta`) represents an output of this step(inference) updated during a streaming response.
As the model generates text token by token, each incremental addition is emitted as a delta, which is later merged into a complete `Message`.

Ailoy provides a simple way to retrieve and aggregate deltas: One can simply use "+"(add) operator.

TBD
