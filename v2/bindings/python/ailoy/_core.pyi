# This file is automatically generated by pyo3_stub_gen
# ruff: noqa: E501, F401

import builtins
import typing
from enum import Enum

CacheResultT = typing.TypeVar("CacheResultT")

class Agent:
    @property
    def lm(self) -> BaseLanguageModel: ...
    @property
    def tools(self) -> list[BaseTool]: ...
    def __new__(cls, lm:BaseLanguageModel, tools:list[BaseTool]=None) -> Agent: ...
    def add_tools(self, tools:list[BaseTool]) -> None: ...
    def add_tool(self, tool:BaseTool) -> None: ...
    def remove_tools(self, tool_names:typing.Sequence[builtins.str]) -> None: ...
    def remove_tool(self, tool_name:builtins.str) -> None: ...
    def run(self, contents:str | list[Part]) -> AgentRunIterator: ...
    def run_sync(self, contents:str | list[Part]) -> AgentRunSyncIterator: ...

class AgentRunIterator:
    def __aiter__(self) -> AgentRunIterator: ...
    def __anext__(self) -> typing.Awaitable[MessageOutput]: ...

class AgentRunSyncIterator:
    def __iter__(self) -> AgentRunSyncIterator: ...
    def __next__(self) -> MessageOutput: ...

class AnthropicLanguageModel(BaseLanguageModel):
    def __new__(cls, model_name:builtins.str, api_key:builtins.str) -> AnthropicLanguageModel: ...
    def run(self, messages:typing.Sequence[Message], tools:typing.Optional[typing.Sequence[ToolDesc]]=None) -> LanguageModelRunIterator: ...
    def run_sync(self, messages:typing.Sequence[Message], tools:typing.Optional[typing.Sequence[ToolDesc]]=None) -> LanguageModelRunSyncIterator: ...

class BaseEmbeddingModel:
    async def run(self, message:builtins.str) -> builtins.list[builtins.float]: ...
    def run_sync(self, message:builtins.str) -> builtins.list[builtins.float]: ...

class BaseLanguageModel:
    def run(self, messages:typing.Sequence[Message], tools:typing.Sequence[ToolDesc]) -> LanguageModelRunIterator: ...
    def run_sync(self, messages:typing.Sequence[Message], tools:typing.Sequence[ToolDesc]) -> LanguageModelRunSyncIterator: ...

class BaseTool:
    @property
    def description(self) -> ToolDesc: ...
    def __call__(self, **kwargs) -> typing.Any:
        r"""
        This is not a function used by Agents, but this let users directly call the tool function for debugging purpose.
        """

class BaseVectorStore:
    def add_vector(self, input:VectorStoreAddInput) -> builtins.str: ...
    def add_vectors(self, inputs:typing.Sequence[VectorStoreAddInput]) -> builtins.list[builtins.str]: ...
    def get_by_id(self, id:builtins.str) -> typing.Optional[VectorStoreGetResult]: ...
    def get_by_ids(self, ids:typing.Sequence[builtins.str]) -> builtins.list[VectorStoreGetResult]: ...
    def retrieve(self, query_embedding:typing.Sequence[builtins.float], top_k:builtins.int) -> builtins.list[VectorStoreRetrieveResult]: ...
    def batch_retrieve(self, query_embeddings:typing.Sequence[typing.Sequence[builtins.float]], top_k:builtins.int) -> builtins.list[builtins.list[VectorStoreRetrieveResult]]: ...
    def remove_vector(self, id:builtins.str) -> None: ...
    def remove_vectors(self, ids:typing.Sequence[builtins.str]) -> None: ...
    def clear(self) -> None: ...
    def count(self) -> builtins.int: ...

class BuiltinTool(BaseTool):
    @property
    def description(self) -> ToolDesc: ...
    @staticmethod
    def terminal() -> BuiltinTool: ...
    def __call__(self, **kwargs) -> typing.Awaitable[list[Part]]: ...
    def __repr__(self) -> builtins.str: ...

class CacheProgress(typing.Generic[CacheResultT]):
    @property
    def comment(self) -> builtins.str: ...
    @property
    def current(self) -> builtins.int: ...
    @property
    def total(self) -> builtins.int: ...
    def __repr__(self) -> builtins.str: ...

class ChromaVectorStore(BaseVectorStore):
    def __new__(cls, chroma_url:builtins.str, collection_name:builtins.str) -> ChromaVectorStore: ...
    def collection_exists(self, collection_name:builtins.str) -> builtins.bool: ...
    def create_collection(self, collection_name:builtins.str, metadata:typing.Optional[dict]=None) -> dict: ...
    def delete_collection(self, collection_name:builtins.str) -> None: ...
    def add_vector(self, input:VectorStoreAddInput) -> builtins.str: ...
    def add_vectors(self, inputs:typing.Sequence[VectorStoreAddInput]) -> builtins.list[builtins.str]: ...
    def get_by_id(self, id:builtins.str) -> typing.Optional[VectorStoreGetResult]: ...
    def get_by_ids(self, ids:typing.Sequence[builtins.str]) -> builtins.list[VectorStoreGetResult]: ...
    def retrieve(self, query_embedding:typing.Sequence[builtins.float], top_k:builtins.int) -> builtins.list[VectorStoreRetrieveResult]: ...
    def batch_retrieve(self, query_embeddings:typing.Sequence[typing.Sequence[builtins.float]], top_k:builtins.int) -> builtins.list[builtins.list[VectorStoreRetrieveResult]]: ...
    def remove_vector(self, id:builtins.str) -> None: ...
    def remove_vectors(self, ids:typing.Sequence[builtins.str]) -> None: ...
    def clear(self) -> None: ...
    def count(self) -> builtins.int: ...

class FaissVectorStore(BaseVectorStore):
    def __new__(cls, dim:builtins.int) -> FaissVectorStore: ...
    def add_vector(self, input:VectorStoreAddInput) -> builtins.str: ...
    def add_vectors(self, inputs:typing.Sequence[VectorStoreAddInput]) -> builtins.list[builtins.str]: ...
    def get_by_id(self, id:builtins.str) -> typing.Optional[VectorStoreGetResult]: ...
    def get_by_ids(self, ids:typing.Sequence[builtins.str]) -> builtins.list[VectorStoreGetResult]: ...
    def retrieve(self, query_embedding:typing.Sequence[builtins.float], top_k:builtins.int) -> builtins.list[VectorStoreRetrieveResult]: ...
    def batch_retrieve(self, query_embeddings:typing.Sequence[typing.Sequence[builtins.float]], top_k:builtins.int) -> builtins.list[builtins.list[VectorStoreRetrieveResult]]: ...
    def remove_vector(self, id:builtins.str) -> None: ...
    def remove_vectors(self, ids:typing.Sequence[builtins.str]) -> None: ...
    def clear(self) -> None: ...
    def count(self) -> builtins.int: ...

class GeminiLanguageModel(BaseLanguageModel):
    def __new__(cls, model_name:builtins.str, api_key:builtins.str) -> GeminiLanguageModel: ...
    def run(self, messages:typing.Sequence[Message], tools:typing.Optional[typing.Sequence[ToolDesc]]=None) -> LanguageModelRunIterator: ...
    def run_sync(self, messages:typing.Sequence[Message], tools:typing.Optional[typing.Sequence[ToolDesc]]=None) -> LanguageModelRunSyncIterator: ...

class LanguageModelRunIterator:
    def __aiter__(self) -> LanguageModelRunIterator: ...
    def __anext__(self) -> typing.Awaitable[MessageOutput]: ...

class LanguageModelRunSyncIterator:
    def __iter__(self) -> LanguageModelRunSyncIterator: ...
    def __next__(self) -> MessageOutput: ...

class LocalEmbeddingModel(BaseEmbeddingModel):
    @classmethod
    def create(cls, model_name:builtins.str, progress_callback:typing.Callable[[CacheProgress], None]=None) -> typing.Awaitable[LocalEmbeddingModel]: ...
    @classmethod
    def create_sync(cls, model_name:builtins.str, progress_callback:typing.Callable[[CacheProgress], None]=None) -> LocalEmbeddingModel: ...
    async def run(self, message:builtins.str) -> builtins.list[builtins.float]: ...
    def run_sync(self, message:builtins.str) -> builtins.list[builtins.float]: ...

class LocalLanguageModel(BaseLanguageModel):
    @classmethod
    def create(cls, model_name:builtins.str, progress_callback:typing.Callable[[CacheProgress], None]=None) -> typing.Awaitable[LocalLanguageModel]: ...
    @classmethod
    def create_sync(cls, model_name:builtins.str, progress_callback:typing.Callable[[CacheProgress], None]=None) -> LocalLanguageModel: ...
    def run(self, messages:typing.Sequence[Message], tools:typing.Optional[typing.Sequence[ToolDesc]]=None) -> LanguageModelRunIterator: ...
    def run_sync(self, messages:typing.Sequence[Message], tools:typing.Optional[typing.Sequence[ToolDesc]]=None) -> LanguageModelRunSyncIterator: ...
    def enable_reasoning(self) -> None: ...
    def disable_reasoning(self) -> None: ...

class MCPTool(BaseTool):
    @property
    def description(self) -> ToolDesc: ...
    def __call__(self, **kwargs) -> typing.Awaitable[list[Part]]: ...
    def __repr__(self) -> builtins.str: ...

class MCPTransport:
    def tools(self, tool_name_prefix:builtins.str) -> typing.Awaitable[list[MCPTool]]: ...
    class Stdio(MCPTransport):
        __match_args__ = ("command", "args",)
        @property
        def command(self) -> builtins.str: ...
        @property
        def args(self) -> builtins.list[builtins.str]: ...
        def __new__(cls, command:builtins.str, args:typing.Sequence[builtins.str]) -> MCPTransport.Stdio: ...
    
    class StreamableHttp(MCPTransport):
        __match_args__ = ("url",)
        @property
        def url(self) -> builtins.str: ...
        def __new__(cls, url:builtins.str) -> MCPTransport.StreamableHttp: ...
    

class Message:
    r"""
    Represents a complete chat message composed of multiple parts (multi-modal).
    
    # OpenAI-compatible shape
    Serialization/deserialization follows the OpenAI chat/response message conventions:
    - `role` matches OpenAI roles (e.g., `"system"`, `"user"`, `"assistant"`, `"tool"`).
    - `content` is an array of typed parts (e.g., `{ "type": "text", "text": "..." }`,
      images, etc.), rather than a single string. This aligns with the modern “array-of-parts”
      format used by OpenAI’s Chat/Responses APIs.
    - `tool_calls` (when present) is compatible with OpenAI function/tool calling:
      each element mirrors OpenAI’s `tool_calls[]` item. In this implementation
      each `tool_calls` entry is stored as a `Part` (commonly `Part::Json`) containing the
      raw JSON payload (`{"type":"function","id":"...","function":{"name":"...","arguments":"..."}}`).
    - `reasoning` is optional and is used to carry model reasoning parts when available from
      reasoning-capable models. Treat it as **auxiliary** content; if your target API
      does not accept a `reasoning` field, omit/strip it before sending.
    
    Because this type supports multi-modal content, exact `Part` variants (text, image, JSON, …)
    may differ from text-only implementations, while staying wire-compatible with OpenAI.
    
    # Fields
    - `role`: Author of the message.
    - `contents`: Primary, user-visible content as a list of parts (text, images, etc.).
    - `reasoning`: Optional reasoning parts (usually text). Not intended for end users.
    - `tool_calls`: Tool/function call requests emitted by the assistant, each stored as a part
      (typically a JSON part containing an OpenAI-shaped `tool_calls[]` item).
    
    # Invariants & recommendations
    - Order is preserved: parts appear in the order they were appended.
    - `reasoning` and `tool_calls` should be present only on assistant messages.
    - If you target strict OpenAI endpoints that don’t accept `reasoning`, drop that field
      during serialization.
    - Parsing/validation of `tool_calls` JSON is the caller’s responsibility.
    
    # Examples
    
    ## User message with text + image
    ```json
    {
      "role": "user",
      "contents": [
        { "type": "text", "text": "What does this sign say?" },
        { "type": "input_image", "image_url": { "url": "https://example.com/sign.jpg" } }
      ]
    }
    ```
    
    ## Assistant message requesting a tool call
    ```json
    {
      "role": "assistant",
      "contents": [ { "type": "text", "text": "I'll look that up." } ],
      "tool_calls": [
        {
          "type": "function",
          "id": "call_abc123",
          "function": {
            "name": "foo",
            "arguments": "{\"location\":\"Dubai\",\"unit\":\"Celsius\"}"
          }
        }
      ]
    }
    ```
    
    ## Assistant message with optional reasoning parts
    ```json
    {
      "role": "assistant",
      "contents": [ { "type": "text", "text": "The current temperature is 42 °C." } ],
      "reasoning": [ { "type": "text", "text": "(model reasoning tokens, if exposed)" } ]
    }
    ```
    """
    @property
    def role(self) -> typing.Optional[Role]: ...
    @property
    def contents(self) -> builtins.list[Part]: ...
    @property
    def reasoning(self) -> builtins.str: ...
    @property
    def tool_calls(self) -> builtins.list[Part]: ...
    @property
    def tool_call_id(self) -> typing.Optional[builtins.str]: ...
    @role.setter
    def role(self, value: Role | typing.Literal["system","user","assistant","tool"]) -> None: ...
    @contents.setter
    def contents(self, value: builtins.list[Part]) -> None: ...
    @reasoning.setter
    def reasoning(self, value: builtins.str) -> None: ...
    @tool_calls.setter
    def tool_calls(self, value: builtins.list[Part]) -> None: ...
    @tool_call_id.setter
    def tool_call_id(self, value: typing.Optional[builtins.str]) -> None: ...
    def __new__(cls) -> Message: ...
    def __repr__(self) -> builtins.str: ...
    def append_contents(self, part:Part) -> None: ...
    def append_tool_call(self, part:Part) -> None: ...

class MessageAggregator:
    r"""
    Incrementally assembles one or more [`Message`]s from a stream of [`MessageDelta`]s.
    
    # What this does
    `MessageAggregator` is a small state machine with a single-item buffer:
    - It **coalesces adjacent chunks of the same kind** to avoid fragmentation
      (e.g., text → text string-append; tool-call JSON → JSON string-append).
    - It **flushes** the buffered chunk into the current [`Message`] whenever a new,
      non-mergeable delta arrives.
    - When the **role changes** (e.g., `assistant` → `tool`), it closes the current
      message and returns it from [`update`], starting a new one for the new role.
    
    This is handy for token-by-token or chunked streaming from LLM APIs, where many
    tiny deltas arrive back-to-back.
    
    # Merge rules (contiguous-only)
    - `Content(Text) + Content(Text)` → append text
    - `Reasoning(Text) + Reasoning(Text)` → append text
    - `ToolCall(Function { id: _, function })`
      + `ToolCall(Function { id: None, function })` → append `function`
    
    Any other case is not merged; instead, the buffer is flushed to the current message.
    
    # Lifecycle
    1. Construct with [`new`].
    2. Feed deltas in order with [`update`].
       - Returns `Some(Message)` **only** when a role boundary is crossed,
         finalizing and yielding the previous role’s message.
       - Returns `None` otherwise.
    3. Call [`finalize`] to flush and retrieve the last in-progress message (if any).
    
    # Guarantees & Notes
    - **Order-preserving**: incorporation order matches arrival order.
    - **Zero parsing**: tool-call JSON is treated as raw text; parse post-aggregation.
    - **Cheap steady-state**: merges use `String::push_str`.
    - **Single-threaded**: not synchronized; use on one task/thread at a time.
    - **Panics**: none expected.
    
    # Example
    ```rust
    # use crate::value::{MessageAggregator, MessageDelta, Part, Role, Message};
    let mut agg = MessageAggregator::new();
    
    // assistant streams content
    assert!(agg.update(MessageDelta::new_assistant_tool_call(Part::Text("Hel".into()))).is_none());
    assert!(agg.update(MessageDelta::new_assistant_content(Part::Text("lo".into()))).is_none());
    
    // role switches to tool: prior assistant message is returned
    let m1 = agg.update(MessageDelta::new_tool_content(Part::Text("ok".into()))).unwrap();
    // ... use m1
    
    // finalize remaining (tool) message
    let m2 = agg.finalize().unwrap();
    // ... use m2
    ```
    """
    @property
    def buffer(self) -> typing.Optional[Message]: ...
    def __new__(cls) -> MessageAggregator: ...
    def update(self, msg_out:MessageOutput) -> typing.Optional[Message]: ...

class MessageOutput:
    @property
    def delta(self) -> Message: ...
    @property
    def finish_reason(self) -> typing.Optional[FinishReason]: ...
    def __repr__(self) -> builtins.str: ...

class OpenAILanguageModel(BaseLanguageModel):
    def __new__(cls, model_name:builtins.str, api_key:builtins.str) -> OpenAILanguageModel: ...
    def run(self, messages:typing.Sequence[Message], tools:typing.Optional[typing.Sequence[ToolDesc]]=None) -> LanguageModelRunIterator: ...
    def run_sync(self, messages:typing.Sequence[Message], tools:typing.Optional[typing.Sequence[ToolDesc]]=None) -> LanguageModelRunSyncIterator: ...

class Part:
    r"""
    Represents one typed unit of message content.
    
    A `Part` is a single element inside a message’s `content` (and, for tools, sometimes
    under `tool_calls`). The enum itself is **transport-agnostic**; any OpenAI-style JSON
    shape is produced/consumed by higher-level (de)serializers.
    
    # Notes
    - No validation is performed. It just store the value as-is.
    """
    @property
    def part_type(self) -> builtins.str: ...
    @property
    def text(self) -> typing.Optional[builtins.str]: ...
    @property
    def function(self) -> typing.Optional[builtins.str]: ...
    @property
    def url(self) -> typing.Optional[builtins.str]: ...
    @property
    def data(self) -> typing.Optional[builtins.str]: ...
    @property
    def mime_type(self) -> typing.Optional[builtins.str]: ...
    @classmethod
    def new_text(cls, _cls:type, text:builtins.str) -> Part: ...
    @classmethod
    def new_image_url(cls, _cls:type, url:builtins.str) -> Part: ...
    @classmethod
    def new_image_data(cls, _cls:type, data:builtins.str, mime_type:builtins.str) -> Part: ...
    @classmethod
    def new_function(cls, _cls:type, id:builtins.str, name:builtins.str, arguments:builtins.str) -> Part: ...
    def __repr__(self) -> builtins.str: ...
    class Text(Part):
        r"""
        Plain UTF-8 text.
        
        Without style, it can be serialized as:
        ```json
        { "type": "text", "text": "hello" }
        ```
        """
        __match_args__ = ("_0",)
        @property
        def _0(self) -> builtins.str: ...
        def __new__(cls, _0:builtins.str) -> Part.Text: ...
        def __len__(self) -> builtins.int: ...
        def __getitem__(self, key:builtins.int) -> typing.Any: ...
    
    class FunctionString(Part):
        r"""
        The **verbatim string** of a tool/function payload as it was streamed/received
        (often a JSON string). This may be incomplete or invalid while streaming. It is
        intended for *as-is accumulation* and later parsing by the caller.
        
        ```json
        "{\"type\": \"function\", \"function\": \"...\""}
        ```
        """
        __match_args__ = ("_0",)
        @property
        def _0(self) -> builtins.str: ...
        def __new__(cls, _0:builtins.str) -> Part.FunctionString: ...
        def __len__(self) -> builtins.int: ...
        def __getitem__(self, key:builtins.int) -> typing.Any: ...
    
    class Function(Part):
        r"""
        A **partially parsed** function.
         - `id`: the `tool_call_id` to correlate results. Use an empty string if undefined.
         - `name`: function/tool name. May be assembled from streaming chunks.
         - `arguments`: raw arguments **string** (typically JSON), preserved verbatim.
        
         Can be mapped to wire formats (e.g., OpenAI `tool_calls[].function`).
        
        ```json
        {
          "id": "call_abc",
          "type": "function",
          "function": { "name": "weather", "arguments": "{ \"city\": \"Paris\" }" }
        }
        ```
        """
        __match_args__ = ("id", "name", "arguments",)
        @property
        def id(self) -> builtins.str: ...
        @property
        def name(self) -> builtins.str: ...
        @property
        def arguments(self) -> builtins.str: ...
        def __new__(cls, id:builtins.str, name:builtins.str, arguments:builtins.str) -> Part.Function: ...
    
    class ImageURL(Part):
        r"""
        A web-addressable image URL (no fetching/validation is performed).
        ```json
        { "type": "image", "url": "https://example.com/cat.png" }
        ```
        """
        __match_args__ = ("_0",)
        @property
        def _0(self) -> builtins.str: ...
        def __new__(cls, _0:builtins.str) -> Part.ImageURL: ...
        def __len__(self) -> builtins.int: ...
        def __getitem__(self, key:builtins.int) -> typing.Any: ...
    
    class ImageData(Part):
        r"""
        Inline base64-encoded image bytes with MIME type (no decoding/validation is performed).
        ```json
        { "type": "image", "data": "<base64>" }
        ```
        """
        __match_args__ = ("data", "mime_type",)
        @property
        def data(self) -> builtins.str: ...
        @property
        def mime_type(self) -> builtins.str: ...
        def __new__(cls, data:builtins.str, mime_type:builtins.str) -> Part.ImageData: ...
    

class PythonAsyncFunctionTool(BaseTool):
    @property
    def description(self) -> ToolDesc: ...
    def __new__(cls, func:typing.Callable[..., typing.Awaitable[typing.Any]], description:ToolDesc) -> PythonAsyncFunctionTool: ...
    def __call__(self, **kwargs) -> typing.Awaitable[list[Part]]: ...
    def __repr__(self) -> builtins.str: ...

class PythonFunctionTool(BaseTool):
    @property
    def description(self) -> ToolDesc: ...
    def __new__(cls, func:typing.Callable[..., typing.Any], description:ToolDesc) -> PythonFunctionTool: ...
    def __call__(self, **kwargs) -> list[Part]:
        r"""
        Unlike another tools, this tool's __call__ is executed synchronously.
        """
    def __repr__(self) -> builtins.str: ...

class ToolDesc:
    @property
    def name(self) -> builtins.str: ...
    @property
    def description(self) -> builtins.str: ...
    @property
    def parameters(self) -> dict: ...
    @property
    def returns(self) -> typing.Optional[dict]: ...
    def __new__(cls, name:builtins.str, description:builtins.str, parameters:dict, *, returns:typing.Optional[dict]=None) -> ToolDesc: ...
    def __repr__(self) -> builtins.str: ...

class VectorStoreAddInput:
    @property
    def embedding(self) -> builtins.list[builtins.float]: ...
    @property
    def document(self) -> builtins.str: ...
    @property
    def metadata(self) -> typing.Optional[dict]: ...
    @embedding.setter
    def embedding(self, value: builtins.list[builtins.float]) -> None: ...
    @document.setter
    def document(self, value: builtins.str) -> None: ...
    @metadata.setter
    def metadata(self, value: typing.Optional[dict]) -> None: ...
    def __new__(cls, embedding:typing.Sequence[builtins.float], document:builtins.str, metadata:typing.Optional[dict]=None) -> VectorStoreAddInput: ...
    def to_dict(self) -> dict: ...

class VectorStoreGetResult:
    @property
    def id(self) -> builtins.str: ...
    @property
    def document(self) -> builtins.str: ...
    @property
    def metadata(self) -> typing.Optional[dict]: ...
    @property
    def embedding(self) -> builtins.list[builtins.float]: ...
    def to_dict(self) -> dict: ...
    def __repr__(self) -> builtins.str: ...

class VectorStoreRetrieveResult:
    @property
    def id(self) -> builtins.str: ...
    @property
    def document(self) -> builtins.str: ...
    @property
    def metadata(self) -> typing.Optional[dict]: ...
    @property
    def distance(self) -> builtins.float: ...
    def to_dict(self) -> dict: ...
    def __repr__(self) -> builtins.str: ...

class XAILanguageModel(BaseLanguageModel):
    def __new__(cls, model_name:builtins.str, api_key:builtins.str) -> XAILanguageModel: ...
    def run(self, messages:typing.Sequence[Message], tools:typing.Optional[typing.Sequence[ToolDesc]]=None) -> LanguageModelRunIterator: ...
    def run_sync(self, messages:typing.Sequence[Message], tools:typing.Optional[typing.Sequence[ToolDesc]]=None) -> LanguageModelRunSyncIterator: ...

class FinishReason(Enum):
    Stop = ...
    Length = ...
    ContentFilter = ...
    ToolCalls = ...

class Role(Enum):
    r"""
    The author of a message (or streaming delta) in a chat.
    """
    System = ...
    r"""
    System instructions and constraints provided to the assistant.
    """
    User = ...
    r"""
    Content authored by the end user.
    """
    Assistant = ...
    r"""
    Content authored by the assistant/model.
    """
    Tool = ...
    r"""
    Outputs produced by external tools/functions, typically in
    response to an assistant tool call (and often correlated via `tool_call_id`).
    """
