"use strict";(self.webpackChunkailoy_docs=self.webpackChunkailoy_docs||[]).push([[783],{4173:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"introducing-ailoy","metadata":{"permalink":"/ailoy/blog/introducing-ailoy","source":"@site/blog/2025-05-25-introducing-ailoy/index.md","title":"Introducing Ailoy","description":"We\u2019re excited to announce the launch of Ailoy, a drop-in library for LLM & agent development.","date":"2025-05-25T00:00:00.000Z","tags":[{"inline":false,"label":"Ailoy","permalink":"/ailoy/blog/tags/ailoy","description":"Ailoy tag description"}],"readingTime":5.175,"hasTruncateMarker":true,"authors":[{"name":"Lee Jaehwan","title":"Brekkylab","page":{"permalink":"/ailoy/blog/authors/jhlee"},"socials":{"github":"https://github.com/jhlee525","linkedin":"https://www.linkedin.com/in/jaehwanlee90/"},"key":"jhlee"}],"frontMatter":{"slug":"introducing-ailoy","title":"Introducing Ailoy","authors":"jhlee","tags":["ailoy"]},"unlisted":false},"content":"We\u2019re excited to announce the launch of **Ailoy**, a drop-in library for LLM & agent development.\\n\\nLarge Language Models (LLMs) and AI agents have recently emerged as a major trend in the software industry, drawing significant attention.\\nYet for many developers, even building a proof of concept can be daunting.\\nIt often requires deep knowledge of machine learning, experience with parallel programming.\\nCloud APIs can ease the burden, but they introduce new cost concerns.\\nTaking it to a production level? That\u2019s even harder.\\n\\n\x3c!-- truncate --\x3e\\n\\n**Ailoy** is designed to solve these challenges.\\nWe focused on on-device AI, which avoids complex setups and cloud costs by running entirely on the local machine.\\nBut to access high-end AI capabilities, cloud APIs still play an important role.\\nOur goal is to keep the right balance between the two.\\n**Ailoy** works seamlessly across cloud, on-device, and even hybrid environments, helping you build and deploy advanced AI functionality with minimal overhead.\\n\\nYou don\u2019t need heavy infrastructure. You don\u2019t need a degree in AI. Just the right tools to get things done.\\n\\n## \ud83d\ude80 What is Ailoy?\\n\\nAiloy is a lightweight library that makes it easy to embed AI into your software.\\nWhether you\'re building a chatbot, an intelligent agent, or enhancing an existing system with LLM capabilities, Ailoy gives you the tools to do it with minimal effort.\\n\\nIt provides:\\n\\n- High-level APIs to run LLMs, including multi-turn conversations, system prompt customization, and reasoning\\n- Built-in tools and functions for constructing agent-based systems, including *Model Context Protocol (MCP)*\\n- Easy agent creation and simplified integration of *Retrieval-Augmented Generation (RAG)*\\n\\nThink of it as your AI development toolkit.\\nIt\'s ideal for embedded assistants, workflow automation, or even building your own reasoning system from the ground up.\\n\\n## \ud83d\udca1 Why Ailoy?\\n\\n**1. Easy to Use**\\n\\nNo need to wrestle with complex orchestration frameworks or deep ML stack knowledge. With Ailoy, you can just import and go.\\nAiloy is designed to feel like any modern developer library.\\n\\n**2. Cloud or On-Device**\\n\\nAiloy supports both cloud vendors like OpenAI and on-device inference using optimized runtimes via TVM.\\nEven better, you can mix both seamlessly in a single application.\\n\\nWant to run models offline to save on API costs? You can.\\nNeed to fall back to the cloud when local resources aren\'t enough? That works too.\\nOr maybe you\'re building a hybrid that combines local tools with remote reasoning \u2014 Ailoy makes that easy.\\nIt\'s also a great fit for secure or privacy-sensitive environments where cloud access isn\'t an option.\\n\\n**3. Versatility Across Architectures**\\n\\nAiloy supports Windows, macOS, and Linux.\\nAnd it works with Python, Node.js and C++ environment.\\n\\nWe\'re continuously expanding our platform and language coverage to make Ailoy available wherever developers build.\\n\\n**4. Developer-Centric Design**\\n\\nWe built Ailoy not only for ML engineers, but also all developers.\\nMany existing LLM frameworks require expertise in ML systems or infrastructure.\\nAiloy aims to break that barrier.\\nWhether you\u2019re writing a script, building a product, or shipping a SaaS, Ailoy lets you focus on your application logic.\\n\\nWe\u2019re currently in the early stages, but iterating rapidly to support the features and workflows real developers care about.\\n\\n## \ud83d\udd0d What You Can Build with Ailoy\\n\\n- A multi-turn chatbot powered by a local model and cloud\\n- A command-line assistant with custom tools\\n- A personalized agent with access to private files via RAG\\n- A desktop app that ships with an embedded LLM\\n\\n## \u26a0\ufe0f Current Limitations\\n\\nWhile Ailoy strives for universality, there are a few constraints to keep in mind.\\n\\nLLMs are still resource-intensive.\\nOn-device execution requires a capable system, and performance can vary significantly across platforms.\\n\\nOn `macOS`, Apple silicon has made it relatively easy to run LLMs locally thanks to it\'s unified memory architecture and `Metal`.\\nHowever, older MacBooks may struggle to meet the performance demands.\\nOn `Windows` and `Linux`, we currently rely on `Vulkan` for LLM execution, where GPU acceleration is often essential for achieving usable inference speeds.\\nAs for mobile devices, we believe it\'s still too early for practical on-device LLM execution due to hardware limitations.\\nInference speed and memory usage depend heavily on the model architecture and the capabilities of the target device.\\n\\nThat said, we\u2019re optimistic about the future.\\nMany hardware vendors are now shipping computers with AI accelerators, and the landscape is quickly evolving.\\nWe\u2019re committed to making Ailoy compatible with these emerging platforms and enabling smooth on-device AI experiences as the hardware ecosystem matures.\\n\\n## \ud83c\udf10 What\u2019s Next\\n\\nWe have a roadmap to make Ailoy even more accessible and powerful.\\n\\nCurrently, we support Alibaba\u2019s `Qwen3` and OpenAI\u2019s `GPT-4o`, and we plan to expand support to a broader range of models, including multimodal ones. \\nWe\'re also working to enable speech recognition and text-to-speech (TTS) models, opening up more possibilities for voice-based AI interactions.\\nIn addition, we\'re bringing Ailoy to the web by supporting client-side execution in modern browsers.\\nLanguage support is also expanding beyond Python and JavaScript, with Java, C#, and other ecosystems on the horizon.\\nWe\u2019re also planning to support inter-machine scenarios.\\nBy building remote model execution capabilities over TCP, Ailoy will enable AI agents to communicate and collaborate across machines.\\nThis will make it possible to build distributed agent systems that can leverage multiple machines working together.\\n\\nOur long-term goal is to make local-first AI the default \u2014 where on-device and cloud-based models work together seamlessly.\\nThis hybrid approach empowers developers to build advanced AI applications while minimizing costs, protecting user privacy, and optimizing performance.\\n\\n## \ud83e\udd1d Join Us on the Journey\\n\\nWe believe accessible AI infrastructure can unlock a whole new class of applications.\\nWhether you\'re an indie developer, a startup team, or part of a large organization, Ailoy is here to help you:\\n\\n- Streamline your development process with intuitive APIs and minimal setup\\n- Build and ship on-device AI applications without worrying about high cloud costs\\n- Run AI in environments where privacy, offline access, or security make cloud services impractical\\n\\nAs we continue expanding Ailoy\u2019s capabilities, we invite you to be part of the journey.\\nYour feedback, ideas, and use cases will shape what Ailoy becomes.\\n\\n**Ready to build?**\\n\\nGrab the package, explore the docs, and start bringing your ideas.\\n\\n[https://github.com/brekkylab/ailoy](https://github.com/brekkylab/ailoy)"}]}}')}}]);