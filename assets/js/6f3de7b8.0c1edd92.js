"use strict";(self.webpackChunkailoy_docs=self.webpackChunkailoy_docs||[]).push([[524],{6649:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"tutorial/rag-using-documents","title":"RAG Using Documents","description":"Retrieval-Augmented Generation (RAG) enables an AI model to use your own","source":"@site/docs/tutorial/rag-using-documents.mdx","sourceDirName":"tutorial","slug":"/tutorial/rag-using-documents","permalink":"/ailoy/docs/tutorial/rag-using-documents","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"documentSidebar","previous":{"title":"MCP Integration","permalink":"/ailoy/docs/tutorial/mcp-integration"},"next":{"title":"Managing Model Files","permalink":"/ailoy/docs/tutorial/managing-model-files"}}');var a=o(4848),i=o(8453);const r={},s="RAG Using Documents",d={},l=[{value:"Step-by-Step Guide",id:"step-by-step-guide",level:2},{value:"Initializing an Embedding Model and a Vector Store",id:"initializing-an-embedding-model-and-a-vector-store",level:3},{value:"Preparing Documents with a Vector Store",id:"preparing-documents-with-a-vector-store",level:3},{value:"Defining the Agent with Knowledge",id:"defining-the-agent-with-knowledge",level:3},{value:"Performing RAG",id:"performing-rag",level:3},{value:"Complete Example",id:"complete-example",level:2},{value:"Output",id:"output",level:2}];function c(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components},{TabItem:o,Tabs:t,TerminalBox:r}=n;return o||g("TabItem",!0),t||g("Tabs",!0),r||g("TerminalBox",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"rag-using-documents",children:"RAG Using Documents"})}),"\n",(0,a.jsx)(n.p,{children:"Retrieval-Augmented Generation (RAG) enables an AI model to use your own\ndocuments as part of its reasoning process. Instead of relying solely on\ninformation learned during model training, RAG allows the model to retrieve\nrelevant knowledge dynamically from external sources."}),"\n",(0,a.jsx)(n.p,{children:"It enables the AI to generate more accurate, up-to-date, and context-aware\nresponses by grounding its answers in your document data."}),"\n",(0,a.jsxs)(n.p,{children:["In ",(0,a.jsx)(n.strong,{children:"Ailoy"}),", RAG is composed of three core components in addition to the\n",(0,a.jsx)(n.code,{children:"Agent"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"EmbeddingModel"})})," \u2014 converts text into vector representations for\nsimilarity search."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"VectorStore"})})," \u2014 stores and retrieves those vectors along with their source\ntext."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"Knowledge"})})," \u2014 a runtime component that performs retrieval and feeds the\nresult into the agent\u2019s prompt."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["In the ",(0,a.jsx)(n.strong,{children:"preparation phase"}),", use the ",(0,a.jsx)(n.code,{children:"EmbeddingModel"})," to encode documents and\npopulate the ",(0,a.jsx)(n.code,{children:"VectorStore"}),". In the ",(0,a.jsx)(n.strong,{children:"runtime phase"}),", just use the ",(0,a.jsx)(n.code,{children:"Knowledge"}),"\nto retrieve relevant documents for each query and include them in the agent\u2019s\ncontext."]}),"\n",(0,a.jsxs)(n.p,{children:["Please refer to the ",(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"../concepts/architecture#knowledge",children:"Architecture"})}),"\nsection for more information."]}),"\n",(0,a.jsx)(n.h2,{id:"step-by-step-guide",children:"Step-by-Step Guide"}),"\n",(0,a.jsx)(n.h3,{id:"initializing-an-embedding-model-and-a-vector-store",children:"Initializing an Embedding Model and a Vector Store"}),"\n","\n",(0,a.jsxs)(t,{groupId:"code-language",children:[(0,a.jsx)(o,{value:"python",label:"Python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:'import ailoy as ai\n\nasync def prepare_knowledge():\n    # create an embedding model\n    model = await ai.EmbeddingModel.new_local(\n        model_name="BAAI/bge-m3",\n        device_id=None,\n        progress_callback=print,  \n    )\n    # create a vector store with dimension 1024\n    vs = ai.VectorStore.new_faiss(dim=1024)\n\nif __name__ == "__main__":\n    asyncio.run(prepare_knowledge())\n'})})}),(0,a.jsx)(o,{value:"javascript",label:"JavaScript",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",metastring:"showLineNumbers",children:'import * as ai from "ailoy-node";\n\nasync function prepare_knowledge() {\n  // create an embedding model\n  const model = await ai.EmbeddingModel.newLocal(\n    "BAAI/bge-m3", // modelName\n    undefined,     // deviceId?\n    console.log    // progressCallback?\n  );\n  // create a vector store with dimension 1024\n  const vs = await ai.VectorStore.newFaiss(1024);\n}\n\nprepare_knowledge().catch((err) => {\n  console.error("Error:", err);\n});\n'})})}),(0,a.jsx)(o,{value:"web",label:"JavaScript(Web)",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",metastring:"web showLineNumbers",children:'import * as ai from "ailoy-web";\n\nasync function prepare_knowledge() {\n  // create an embedding model\n  const model = await ai.EmbeddingModel.newLocal(\n    "BAAI/bge-m3", // modelName\n    undefined,     // deviceId?\n    console.log    // progressCallback?\n  );\n  // create a vector store with dimension 1024\n  const vs = await ai.VectorStore.newFaiss(1024);\n}\n\nprepare_knowledge().catch((err) => {\n  console.error("Error:", err);\n});\n'})})})]}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsxs)(n.p,{children:["At this time, the only supported embedding model is ",(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"https://huggingface.co/BAAI/bge-m3",children:(0,a.jsx)(n.code,{children:"BAAI/bge-m3"})})}),". Additional embedding models will be supported in future releases."]})}),"\n","\n",(0,a.jsx)(n.h3,{id:"preparing-documents-with-a-vector-store",children:"Preparing Documents with a Vector Store"}),"\n",(0,a.jsx)(n.p,{children:"Before running RAG, create a vector store to store document embeddings for\nretrieval. This step prepares your data for semantic search and typically only\nneeds to be performed once per dataset."}),"\n","\n",(0,a.jsxs)(t,{groupId:"code-language",children:[(0,a.jsx)(o,{value:"python",label:"Python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:'import asyncio\n\nimport ailoy as ai\n\nCHUNKS = [...]\n\nasync def prepare_knowledge() -> ai.Knowledge:\n    model = await ai.EmbeddingModel.new_local("BAAI/bge-m3")\n    vs = ai.VectorStore.new_faiss(1024)\n    for i, chunk in enumerate(CHUNKS):\n        # embed chunks into semantic vectors\n        embedding = await model.infer(chunk)\n        # store semantic vectors\n        vs.add_vector(\n            ai.VectorStoreAddInput(\n                embedding,\n                chunk,\n                {"title": "The Old Man and the Sea", "index": i}\n            )\n        )\n\n    # create a knowledge based on the vector store\n    return ai.Knowledge.new_vector_store(vs, model)\n\nif __name__ == "__main__":\n    asyncio.run(prepare_knowledge())\n'})})}),(0,a.jsx)(o,{value:"javascript",label:"JavaScript",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",metastring:"showLineNumbers",children:'import * as ai from "ailoy-node";\n\nconst CHUNKS = [...];\n\nasync function prepare_knowledge() {\n  const model = await ai.EmbeddingModel.newLocal("BAAI/bge-m3");\n  const vs = await ai.VectorStore.newFaiss(1024);\n  for (const [i, chunk] of CHUNKS.entries()) {\n    // embed chunks into semantic vectors\n    const embedding = await model.infer(chunk);\n    // store semantic vectors\n    await vs.addVector({\n      embedding: embedding,\n      document: chunk,\n      metadata: {\n        title: "The Old Man and the Sea",\n        index: i,\n      },\n    });\n  }\n\n  // create a knowledge based on the vector store\n  return ai.Knowledge.newVectorStore(vs, model);\n}\n\nprepare_knowledge().catch((err) => {\n  console.error("Error:", err);\n});\n'})})}),(0,a.jsx)(o,{value:"web",label:"JavaScript(Web)",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",metastring:"web showLineNumbers",children:'import * as ai from "ailoy-web";\n\nconst CHUNKS = [...];\n\nasync function prepare_knowledge() {\n  const model = await ai.EmbeddingModel.newLocal("BAAI/bge-m3");\n  const vs = await ai.VectorStore.newFaiss(1024);\n  for (const [i, chunk] of CHUNKS.entries()) {\n    // embed chunks into semantic vectors\n    const embedding = await model.infer(chunk);\n    // store semantic vectors\n    await vs.addVector({\n      embedding: embedding,\n      document: chunk,\n      metadata: {\n        title: "The Old Man and the Sea",\n        index: i,\n      },\n    });\n  }\n\n  // create a knowledge based on the vector store\n  return ai.Knowledge.newVectorStore(vs, model);\n}\n\nprepare_knowledge().catch((err) => {\n  console.error("Error:", err);\n});\n\n'})})})]}),"\n","\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["Ailoy currently supports both\n",(0,a.jsx)(n.a,{href:"https://github.com/facebookresearch/faiss",children:(0,a.jsx)(n.strong,{children:"FAISS"})})," and\n",(0,a.jsx)(n.a,{href:"https://www.trychroma.com/",children:(0,a.jsx)(n.strong,{children:"ChromaDB"})})," as vector store backends. Refer to\nthe official configuration guide for backend-specific options."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"defining-the-agent-with-knowledge",children:"Defining the Agent with Knowledge"}),"\n",(0,a.jsxs)(n.p,{children:["You can now create an ",(0,a.jsx)(n.code,{children:"Agent"})," with the ",(0,a.jsx)(n.code,{children:"Knowledge"})," module that integrates your\nvector store and embedding model."]}),"\n","\n",(0,a.jsxs)(t,{groupId:"code-language",children:[(0,a.jsx)(o,{value:"python",label:"Python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:'import asyncio\n\nimport ailoy as ai\n\n\nasync def prepare_knowledge():\n    ...\n\n\nasync def main(knowledge: ai.Knowledge):\n    # create an agent with knowledge\n    agent = ai.Agent(\n        await ai.LangModel.new_local("Qwen/Qwen3-0.6B"),\n        knowledge=knowledge,\n  )\n\nif __name__ == "__main__":\n    knowledge = asyncio.run(prepare_knowledge())\n    asyncio.run(main(knowledge))\n'})})}),(0,a.jsx)(o,{value:"javascript",label:"JavaScript",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",metastring:"showLineNumbers",children:'import * as ai from "ailoy-node";\n\nasync function prepare_knowledge() {...}\n\nasync function main(knowledge: ai.Knowledge) {\n  // create an agent with knowledge\n  const agent = new ai.Agent(\n    await ai.LangModel.newLocal("Qwen/Qwen3-0.6B"),\n    undefined,  // tools (not used here)\n    knowledge   // knowledge\n  );\n}\n\nprepare_knowledge()\n  .then((knowledge) => main(knowledge))\n  .catch((err) => {\n    console.error("Error:", err);\n  });\n'})})}),(0,a.jsx)(o,{value:"web",label:"JavaScript(Web)",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",metastring:"web showLineNumbers",children:'import * as ai from "ailoy-web";\n\nasync function prepare_knowledge() {...}\n\nasync function main(knowledge: ai.Knowledge) {\n  // create an agent with knowledge\n  const agent = new ai.Agent(\n    await ai.LangModel.newLocal("Qwen/Qwen3-0.6B"),\n    undefined,  // tools (not used here)\n    knowledge   // knowledge\n  );\n}\n\nprepare_knowledge()\n  .then((knowledge) => main(knowledge))\n  .catch((err) => {\n    console.error("Error:", err);\n  });\n'})})})]}),"\n","\n",(0,a.jsx)(n.h3,{id:"performing-rag",children:"Performing RAG"}),"\n",(0,a.jsx)(n.p,{children:"To perform retrieval and generate grounded responses:"}),"\n","\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsxs)(n.p,{children:["Not all models supports ",(0,a.jsx)(n.code,{children:"documents"})," natively.",(0,a.jsx)(n.br,{}),"\n","To enable retrieval-based reasoning in these models, make sure that\napplying the ",(0,a.jsx)(n.strong,{children:"document polyfill"})," which adapts the agent\u2019s prompt\nstructure to include retrieved documents."]})}),"\n",(0,a.jsxs)(t,{groupId:"code-language",children:[(0,a.jsx)(o,{value:"python",label:"Python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:'import asyncio\n\nimport ailoy as ai\n\n\nasync def prepare_knowledge():\n    ...\n\n\nasync def main(knowledge: ai.Knowledge):\n    agent = ai.Agent(\n        await ai.LangModel.new_local("Qwen/Qwen3-0.6B"),\n        knowledge=knowledge,\n    )\n    config = ai.AgentConfig(\n        # need polyfill here because Qwen3 doesn\'t natively support \'documents\'\n        inference=ai.InferenceConfig(document_polyfill=ai.get_qwen3_polyfill())\n        # set this to use `top_k` documents with high similarity. default `top_k`=1\n        knowledge=ai.KnowledgeConfig(top_k=1)\n    )\n    async for resp in agent.run("Why did the boy stop fishing with the old man?", config):\n        print(resp.message.contents[0].text)\n\nif __name__ == "__main__":\n    knowledge = asyncio.run(prepare_knowledge())\n    asyncio.run(main(knowledge))\n'})})}),(0,a.jsx)(o,{value:"javascript",label:"JavaScript",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",metastring:"showLineNumbers",children:'import * as ai from "ailoy-node";\n\nasync function prepare_knowledge() {...}\n\nasync function main(knowledge: ai.Knowledge) {\n  const agent = new ai.Agent(\n    await ai.LangModel.newLocal("Qwen/Qwen3-0.6B"),\n    undefined,\n    knowledge\n  );\n  const config = { \n    // need polyfill here because Qwen3 doesn\'t natively support \'documents\'\n    inference: { documentPolyfill: ai.getQwen3Polyfill() },\n    // set this to use `topK` documents with high similarity. default `topK`=1\n    knowledge: { topK: 1 }\n  };\n  for await (const resp of agent.run(\n    "Why did the boy stop fishing with the old man?",\n    config\n  )) {\n    if (resp.message.contents?.[0]?.type === "text")\n      console.log(resp.message.contents?.[0]?.text);\n  }\n}\n\nprepare_knowledge()\n  .then((knowledge) => main(knowledge))\n  .catch((err) => {\n    console.error("Error:", err);\n  });\n'})})}),(0,a.jsx)(o,{value:"web",label:"JavaScript(Web)",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",metastring:"web showLineNumbers",children:'import * as ai from "ailoy-web";\n\nasync function prepare_knowledge() {...}\n\nasync function main(knowledge: ai.Knowledge) {\n  const agent = new ai.Agent(\n    await ai.LangModel.newLocal("Qwen/Qwen3-0.6B"),\n    undefined,\n    knowledge\n  );\n  const config = { \n    // need polyfill here because Qwen3 doesn\'t natively support \'documents\'\n    inference: { documentPolyfill: ai.getQwen3Polyfill() },\n    // set this to use `topK` documents with high similarity. default `topK`=1\n    knowledge: { topK: 1 }\n  };\n  for await (const resp of agent.run(\n    "Why did the boy stop fishing with the old man?",\n    config\n  )) {\n    if (resp.message.contents?.[0]?.type === "text")\n      console.log(resp.message.contents?.[0]?.text);\n  }\n}\n\nprepare_knowledge()\n  .then((knowledge) => main(knowledge))\n  .catch((err) => {\n    console.error("Error:", err);\n  });\n'})})})]}),"\n","\n",(0,a.jsx)(n.h2,{id:"complete-example",children:"Complete Example"}),"\n","\n",(0,a.jsxs)(t,{groupId:"code-language",children:[(0,a.jsx)(o,{value:"python",label:"Python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:'import asyncio\n\nimport ailoy as ai\n\nCHUNKS = [\n    """\nHe was an old man who fished alone in a skiff in the Gulf Stream and he had gone\neighty-four days now without taking a fish. In the first forty days a boy had been with him.\nBut after forty days without a fish the boy\u2019s parents had told him that the old man was\nnow definitely and finally salao, which is the worst form of unlucky, and the boy had gone\nat their orders in another boat which caught three good fish the first week. It made the\nboy sad to see the old man come in each day with his skiff empty and he always went\ndown to help him carry either the coiled lines or the gaff and harpoon and the sail that\nwas furled around the mast. The sail was patched with flour sacks and, furled, it looked\nlike the flag of permanent defeat.\n""",\n    """\nThe old man was thin and gaunt with deep wrinkles in the back of his neck. The\nbrown blotches of the benevolent skin cancer the sun brings from its [9] reflection on the\ntropic sea were on his cheeks. The blotches ran well down the sides of his face and his\nhands had the deep-creased scars from handling heavy fish on the cords. But none of\nthese scars were fresh. They were as old as erosions in a fishless desert.\n""",\n    """\nEverything about him was old except his eyes and they were the same color as the\nsea and were cheerful and undefeated.\n\u201cSantiago,\u201d the boy said to him as they climbed the bank from where the skiff was\nhauled up. \u201cI could go with you again. We\u2019ve made some money.\u201d\nThe old man had taught the boy to fish and the boy loved him.\n\u201cNo,\u201d the old man said. \u201cYou\u2019re with a lucky boat. Stay with them.\u201d\n""",\n]\n\nasync def prepare_knowledge():\n    model = await ai.EmbeddingModel.new_local("BAAI/bge-m3")\n    vs = ai.VectorStore.new_faiss(1024)\n    for i, chunk in enumerate(CHUNKS):\n        embedding = await model.infer(chunk)\n        vs.add_vector(\n            ai.VectorStoreAddInput(\n                embedding, chunk, {"title": "The Old Man and the Sea", "index": i}\n            )\n        )\n\n    return ai.Knowledge.new_vector_store(vs, model)\n\n\nasync def main(knowledge: ai.Knowledge):\n    agent = ai.Agent(\n        await ai.LangModel.new_local("Qwen/Qwen3-0.6B"),\n        knowledge=knowledge,\n    )\n    config = ai.AgentConfig(\n        inference=ai.InferenceConfig(document_polyfill=ai.get_qwen3_polyfill())\n    )\n    async for resp in agent.run("Why did the boy stop fishing with the old man?", config):\n        print(resp.message.contents[0].text)\n\n\nif __name__ == "__main__":\n    knowledge = asyncio.run(prepare_knowledge())\n    asyncio.run(main(knowledge))\n'})})}),(0,a.jsx)(o,{value:"javascript",label:"JavaScript",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",metastring:"showLineNumbers",children:'import * as ai from "ailoy-node";\n\nconst CHUNKS = [\n  `\nHe was an old man who fished alone in a skiff in the Gulf Stream and he had gone\neighty-four days now without taking a fish. In the first forty days a boy had been with him.\nBut after forty days without a fish the boy\u2019s parents had told him that the old man was\nnow definitely and finally salao, which is the worst form of unlucky, and the boy had gone\nat their orders in another boat which caught three good fish the first week. It made the\nboy sad to see the old man come in each day with his skiff empty and he always went\ndown to help him carry either the coiled lines or the gaff and harpoon and the sail that\nwas furled around the mast. The sail was patched with flour sacks and, furled, it looked\nlike the flag of permanent defeat.\n`,\n  `\nThe old man was thin and gaunt with deep wrinkles in the back of his neck. The\nbrown blotches of the benevolent skin cancer the sun brings from its [9] reflection on the\ntropic sea were on his cheeks. The blotches ran well down the sides of his face and his\nhands had the deep-creased scars from handling heavy fish on the cords. But none of\nthese scars were fresh. They were as old as erosions in a fishless desert.\n`,\n  `\nEverything about him was old except his eyes and they were the same color as the\nsea and were cheerful and undefeated.\n\u201cSantiago,\u201d the boy said to him as they climbed the bank from where the skiff was\nhauled up. \u201cI could go with you again. We\u2019ve made some money.\u201d\nThe old man had taught the boy to fish and the boy loved him.\n\u201cNo,\u201d the old man said. \u201cYou\u2019re with a lucky boat. Stay with them.\u201d\n`,\n];\n\nasync function prepare_knowledge() {\n  const model = await ai.EmbeddingModel.newLocal("BAAI/bge-m3");\n  const vs = await ai.VectorStore.newFaiss(1024);\n  for (const [i, chunk] of CHUNKS.entries()) {\n    const embedding = await model.infer(chunk);\n    await vs.addVector({\n      embedding: embedding,\n      document: chunk,\n      metadata: {\n        title: "The Old Man and the Sea",\n        index: i,\n      },\n    });\n  }\n\n  return ai.Knowledge.newVectorStore(vs, model);\n}\n\nasync function main(knowledge: ai.Knowledge) {\n  const agent = new ai.Agent(\n    await ai.LangModel.newLocal("Qwen/Qwen3-0.6B"),\n    undefined,\n    knowledge\n  );\n\n  const config = { inference: { documentPolyfill: ai.getQwen3Polyfill() } };\n  for await (const resp of agent.run(\n    "Why did the boy stop fishing with the old man?",\n    config\n  )) {\n    if (resp.message.contents?.[0]?.type === "text")\n      console.log(resp.message.contents?.[0]?.text);\n  }\n}\n\nprepare_knowledge()\n  .then((knowledge) => main(knowledge))\n  .catch((err) => {\n    console.error("Error:", err);\n  });\n'})})}),(0,a.jsx)(o,{value:"web",label:"JavaScript(Web)",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",metastring:"web showLineNumbers",children:'import * as ai from "ailoy-web";\n\nconst CHUNKS = [\n  `\nHe was an old man who fished alone in a skiff in the Gulf Stream and he had gone\neighty-four days now without taking a fish. In the first forty days a boy had been with him.\nBut after forty days without a fish the boy\u2019s parents had told him that the old man was\nnow definitely and finally salao, which is the worst form of unlucky, and the boy had gone\nat their orders in another boat which caught three good fish the first week. It made the\nboy sad to see the old man come in each day with his skiff empty and he always went\ndown to help him carry either the coiled lines or the gaff and harpoon and the sail that\nwas furled around the mast. The sail was patched with flour sacks and, furled, it looked\nlike the flag of permanent defeat.\n`,\n  `\nThe old man was thin and gaunt with deep wrinkles in the back of his neck. The\nbrown blotches of the benevolent skin cancer the sun brings from its [9] reflection on the\ntropic sea were on his cheeks. The blotches ran well down the sides of his face and his\nhands had the deep-creased scars from handling heavy fish on the cords. But none of\nthese scars were fresh. They were as old as erosions in a fishless desert.\n`,\n  `\nEverything about him was old except his eyes and they were the same color as the\nsea and were cheerful and undefeated.\n\u201cSantiago,\u201d the boy said to him as they climbed the bank from where the skiff was\nhauled up. \u201cI could go with you again. We\u2019ve made some money.\u201d\nThe old man had taught the boy to fish and the boy loved him.\n\u201cNo,\u201d the old man said. \u201cYou\u2019re with a lucky boat. Stay with them.\u201d\n`,\n];\n\nasync function prepare_knowledge() {\n  const model = await ai.EmbeddingModel.newLocal("BAAI/bge-m3");\n  const vs = await ai.VectorStore.newFaiss(1024);\n  for (const [i, chunk] of CHUNKS.entries()) {\n    const embedding = await model.infer(chunk);\n    await vs.addVector({\n      embedding: embedding,\n      document: chunk,\n      metadata: {\n        title: "The Old Man and the Sea",\n        index: i,\n      },\n    });\n  }\n\n  return ai.Knowledge.newVectorStore(vs, model);\n}\n\nasync function main(knowledge: ai.Knowledge) {\n  const agent = new ai.Agent(\n    await ai.LangModel.newLocal("Qwen/Qwen3-0.6B"),\n    undefined,\n    knowledge\n  );\n\n  const config = { inference: { documentPolyfill: ai.getQwen3Polyfill() } };\n  for await (const resp of agent.run(\n    "Why did the boy stop fishing with the old man?",\n    config\n  )) {\n    if (resp.message.contents?.[0]?.type === "text")\n      console.log(resp.message.contents?.[0]?.text);\n  }\n}\n\nprepare_knowledge()\n  .then((knowledge) => main(knowledge))\n  .catch((err) => {\n    console.error("Error:", err);\n  });\n'})})})]}),"\n","\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsx)(n.p,{children:"For best results, ensure your documents are chunked semantically (e.g., by paragraphs or sections)."})}),"\n","\n",(0,a.jsx)(n.h2,{id:"output",children:"Output"}),"\n","\n",(0,a.jsx)(r,{children:(0,a.jsx)(n.p,{children:"The boy stopped fishing with the old man because he was with a lucky boat, not because he was in possession of a boat or because he was with the old man. The boy was with a boat, not a person. The old man had taught the boy to fish, and the boy loved him, but the reason for stopping fishing was related to the boat."})}),"\n"]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}function g(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}}}]);